---
title: "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines"
format: 
    revealjs:
        theme: moon
editor: visual
authors: 
  - name: Josh Hollandsworth
  - name: Brad Lipson
  - name: Eric Miller
date: last-modified
self-contained: true
execute: 
  warning: false
  message: false
params:
  nu_gamma: 0.02
  nu_nu: 0.75
  nu_challenger_gamma: 0.17
  nu_challenger_nu: 0.87
  c_gamma: 0.01
  c_cost: 0.01
  c_challenger_gamma: 0.14
  c_challenger_cost: 0.1
  rand_seed: 12341234
  final_rand_seed: 1923904
---

```{r}
#| label: package-install
#| echo: false


# Load all the packages and our base data set
library(easypackages)
dep_packages = c("tidyverse",
    "ggthemes",
    "ggrepel",
    "dslabs", 
    "ggplot2",
    "graphics",
    "dplyr",
    "data.table",
    "extrafont",
    "dataMaid",
    "gt",
    "gtsummary",
    "gtExtras",
    "mlr",
    "e1071",
    "caret",
    "caTools",
    "MLmetrics",
    "pROC",
    "DMwR",
    "doParallel",
    "foreach"

)
easypackages::libraries(dep_packages)

mimic_data <- fread("./MIMIC_ICU_Data/mimic_icu_data.csv")

# Create a subset of the entire data set to just include the rows we have determined are of interest
# convert the gender field to an `is_male` flag. Also discretize the heart rate and age
model_data <- mimic_data %>%
  select(
    icustay_id,
    age,
    gender,
    hospital_expire_flag,
    heartrate_mean,
    sysbp_mean,
    resprate_mean,
    tempc_mean,
    wbc_mean,
    platelet_min,
    creatinine_max,
    lactate_mean
  ) %>%
  mutate(
    is_male = factor(case_when(gender == "M" ~ 1, TRUE ~ 0)),
    age_range = factor(
      case_when(
        age <= 18 ~ "<=18",
        age > 18 & age <= 40 ~ "19 to 40",
        age > 40 & age <= 60 ~ "41 to 60",
        age > 60 ~ ">=61"
      )
    ),
    heart_rate = factor(
      case_when(
        heartrate_mean < 60 ~ "<60",
        heartrate_mean >= 61 & heartrate_mean <= 80 ~ "60 to 80",
        heartrate_mean >= 81 & heartrate_mean <= 100 ~ "81 to 100",
        heartrate_mean > 100 ~ "above 100"
      )
    ),
    survival = factor(case_when(
      hospital_expire_flag == 1 ~ "DIED", TRUE ~ "SURVIVED"
    ))
  ) %>%
  select(-gender,-heartrate_mean,-age,-hospital_expire_flag) %>%
  drop_na()
```

# SVM's and Our Data Source

With Eric Miller

# History and Background

-   Developed by Vapnik and Chervonenkis in 1964 then revised in 1992 to incorporate Non-linear classifiers
-   Support Vector Machines (SVM) employs supervised learning using past data to train models for new cases
-   SVM models are used to form clusters of two distinct data groups
-   SVM establishes a hyperplane to maximize the margin between data groups
-   The hyperplane acts as a line separating the clusters
-   The goal is to ensure the greatest separation between neighboring members of each cluster

# Data Source

-   Our team is utilizing data from the Medical Information Mart for Intensive Care (MIMIC)
-   We are focusing on the MIMIC III dataset.
-   The dataset was pre-processed to isolate the most statistically significant features

::: notes
Additional information about the MIMIC III dataset can be found online at the Massachusetts Institute of Technologies website linked in the paper. https://mimic.mit.edu/docs/iii/.
:::

# Methods - Kernal Function

-   Support Vector Machines operate on both linear and non-linear data by using a kernel function or "kernel trick" to manipulate non-linear data into a linear space for classification. The general formula for a kernel function is as follows, where $X_i, X_j$ is a tuple.

$$
K(X_i, X_j) = \Phi(X_i)\Phi(X_j)  
$$

# Methods - Hyperplane

Once the data is linearly separable, we can define our maximally marginal hyperplane. A general formula for the hyperplane is as follows $$
W \times X + b = 0
$$

This formula has 2 components of import. The first is a $W$, a weight vector and $b$ a scalar bias.

Since $W$ is a simple weighting vector it would be in the form of the below

$$
W = \{w_1, w_2, \dots, w_n \}
$$

# Methods - Support Vectors

The tuples that lie the closest to the margins of the maximal marginal hyperplane are the actual support vectors.

Using the formulas on the previous slide, if our support vectors where at $y_i = 1$ and $y_i = -1$ our hyperplane margins would be defined as

$$
H_1: W_0 + W_{1}X_{1} + W_{2}X_{2} + \dots + W_{n}X_{n} \ge 1
$$

and

$$
H_2: W_0 + W_{1}X_{1} + W_{2}X_{2} + \dots + W_{n}X_{n} \le -1
$$

# SVM Vizualization

![](presentation_images/svm.jpeg)

::: footer
Source https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8
:::

# Modelling ICU Patient survival from the MIMIC dataset

With Josh Hollandsworth

::: notes
Hello all, I'm Josh and im going to walk you through the our journey developing a model to predict ICU Patient Survival

We started with 12 features that we thought would play an important role in predicting patient survival We also wanted to see if the our full feature set was necessary or if we could build a reduced model that trained quicker while supplying siimilar out comes. To facilitate this we built a challenger model

Of course the most important part of modelling was kernel selection and tuning which we will jump into next.
:::

## Important Concepts

-   used the e1071 r package
-   leveraged the `tune(svm, ...)` function for tuning our model hyperparameters

::: notes
However before we go much further lets get some basic terminology out of the way and outline some technological decisions we made.

First we selected the e1071 r package to build, train, and test our model. There are alternatives out there however e1071 appears to be the most popular

Secondly we didn't really know what the best hyperparameters or even kernel where so we leveraged the tune method to try a bunch of tunings to see what produced the best accuracy
:::

## Our intial primary model

-   was of type "C-Classification"
-   leveraged a linear kernel
-   only tuned the cost hyperparameter

::: notes
we selected C-classification which is the default because it selects the response variable as a binary classifier we also tested a linear kernel because nothing jumped out at us when doing data exploration that our data would not be easily linearly seperable we tuned cost which is the penalty for an incorrect prediction. we did this by passing a list of cost values from 0.001 to 100 with each step being a factor of 10
:::

```{r}
#| label: execute-initial-model


#create survival factor of levels for graphing
survivalLevels <- attributes(model_data$survival)$levels

# set a random seed so that this is repeatable.
set.seed(params$rand_seed)

# do a basic 80/20 split of the data
train <- model_data %>% dplyr::sample_frac(0.8)
test  <- dplyr::anti_join(model_data, train, by = 'icustay_id')

# bulid our model
model <- svm(survival ~ . -icustay_id, data = train, kernel = "linear", cost = 0.01, scale = TRUE, probability=TRUE)

# perform the predictions on the test set, and include probabilities
iPredictions <- predict(model, test, probability=TRUE)

# exteact probabilities from the predictions
iProbabilities <- attr(iPredictions, "probabilities")[,1]

# build a confusion matrix
iConfusionMatrix <- confusionMatrix(iPredictions, test$survival, mode = "everything", positive="SURVIVED")
```

## Initial Model Analysis

::: panel-tabset
### ROC Cuve with AUC

```{r}
#| label: initial-model-roc-auc

# calculate a ROC and create a performance object with tpr (true positive rate) over fpr (false positive rate)
iROC <- pROC::roc(as.factor(test$survival), iProbabilities)
iROCRPredictions <- ROCR::prediction(iProbabilities, test$survival)
iPerformance <- ROCR::performance(iROCRPredictions, "tpr", "fpr")

# plot peformance, with a diagonal 50 percent line and the AUC and F1 Score in the header
plot(iPerformance, colorize = FALSE, main="ROC Curve for C-Classification")
abline(a=0,b=1)
mtext(paste("AUC =",round(as.numeric(iROC$auc), 4), " F1 Score = ", round(as.numeric( iConfusionMatrix$byClass["F1"]),4)))
```

### Confusion Matrix

```{r}
#| label: initial-model-confustion-matrix
fourfoldplot(as.table(iConfusionMatrix), color=c("navy", "lightblue"), main="C-classification Confusion Matrix")
```
:::

::: notes
On our ROC Curve we can see that our model has roughly a 74% accuracy as that is the area under the ROC Curve. Ideally that number would be higher but it was acceptable as anything above the 50% diagonal line means that our model is more accurate than flipping a coin

<go to confusion matrix>

Our confusion matrix however was a disaster. As you can see, despite having an acceptable accuracy, our model was very optimistic and predicted that everyone who entered ICU would survive. While we would love for this to match reality, we know its not true. Additionally a model that assumes everyone survives is of little value given
:::

# What was wrong with our model

-   Class imbalance !!!
    -   86% of the time, patients surived
    -   14% of the time, patients died
-   Random sampling could exacerbate this problem

::: notes
Our problem was that we had class imbalance. It seems to follow reason that a lot of patients who enter ICU will survive, they just need higher levels of monitoring and care. However of course this is not always the case. The end result was that our data set was majorily comprised of patients sruviving.

This problem could be exacerbated by random sampling which could chose an even smaller percentage of the minority case(patient death)
:::

# Fixing the model

We attempted 2 strategies to fix the

-   Oversampling of minority case
    -   Ensure that minority case was a larger percentage of the training set via selection with replacement
-   Downsampling majority case
    -   Take a large percentage of the minority cases, select and EQUIVALENT number of majority cases for a 50/50 split

::: notes
First we attempted to oversample our minority case by selecting ensuring that they met a certain precentage of the train data set and allowed random sampling to occur with replacement (meaning we could select the same observation multiple times) This improved the model but not significantly Secondly we downsampled the majority case by selecting 95% of the minority case, then randomly selecting and EQUIVALENT number of the majority cases for a 50/50 split This proved better than oversampling, but still wasnt optimal
:::

# Tuning the new model

-   tested using a rbf (radial bias kernel)
-   switched to nu-classification
-   tuned for nu and gamma
    -   using grid search strategy and a bunch of packages

::: notes
when we tuned we decided to try a few other options of the e1071 package. First we switched to a radial basis kernel. R secondly we switched to nu classification. Which according the documentation is the same as C classification but it constrains the nu value between 0 and 1, nu its self is related to the ratio of support vectors and the ratio of the training error we now had to tune for nu and gamma hyperparemeters we had no clue in how to do this and since our model took a large hit in accuracy by downsampling we wanted to try a bunch of things and select the best to do this we leveraged a grid search strategy which is effectively a brute force attemp at finding hyperparameters
:::

# Tuning the new model...more problems

-   Tune SVM is VERY VERY SLOW
    -   does not leverage mulitiple cpu cores
    -   operates sequentially
-   doParallel and foreach to the rescue
-   still slow but much quicker at the same time

::: notes
given we had two hyperparameters to tune and wanted the best accuracy we attempted to use grid search with tune svm. I set this up on my machine, walked away, came back a few hours later and nothing had completed. eventually my machine crashed Discovered do parallel and foreach to allow me to pass the variables in and build and validate models, aggregate results and pick the best parameters
:::

# Tuning the new model and paying more for it

![](presentation_images/tuning-cpu-usage.jpeg)

::: notes
This is a fun graph to show the out comes, this is on a an 8 core 10th gen intel cpu. Training took around 45 minutes for the nu classification and 20 minutes c-classification

My CPU ran near 96 degrees celsius \@ 4.8 ghz for the duration of the test
:::

# Tuning Results

-   nu-classification won out
-   Primary Model
    -   Gamma = `r params$nu_gamma`
    -   Nu = `r params$nu_nu`
-   Challenger model
    -   Gamma = `r params$nu_challenger_gamma`
    -   Nu = `r params$nu_challenger_nu`

# Model results (Primary)

```{r}
#| label: primary-model-results
#| echo: false

# perform zscore normalization 
scaled_model_data <- model_data %>%
  select(-icustay_id) %>%
  mutate_if(is.numeric, scale) %>%
  cbind(icustay_id = model_data$icustay_id)

#create survival factor of levels for graphing
survivalLevels <- attributes(model_data$survival)$levels

# set a random seed so that this is repeatable.
set.seed(params$final_rand_seed)

# our data are imbalanced such that the patients tend to survive (hospital expire flag = 0), to resolve this issue we will downsample 
# the surviving patients and construct a training set based on an 80% selection of all of the survival == "DIED"
# then randomly select and equal number of observations where survival == "DIED"
amount_to_sample = floor(sum(scaled_model_data$survival == "DIED") * 0.95)
train <-
  scaled_model_data %>% group_by(survival) %>% sample_n(size = amount_to_sample) %>% ungroup()
test <- scaled_model_data %>% anti_join(train, by = "icustay_id")

set.seed(params$final_rand_seed)
nuModel <- e1071::svm(survival ~ . - icustay_id,
                      type="nu-classification",
                      kernel="radial",
                      data=train,
                      probability=TRUE,
                      gamma=params$nu_gamma,
                      nu=params$nu_nu,
                      scale=TRUE)

nuPredictions <- predict(nuModel, test, probability=TRUE)
nuProbabilities <- attr(nuPredictions, "probabilities")[,1]
nuROC <- pROC::roc(as.factor(test$survival), nuProbabilities)
nuConfusionMatrix <- confusionMatrix(nuPredictions, test$survival, mode="everything", positive = "SURVIVED")

#to appropriately build the roc graph, buid a label order based on survivalLevels and the positive case from the confusion matrix
nuInverse <- c( nuConfusionMatrix$positive, survivalLevels[which(survivalLevels != nuConfusionMatrix$positive)])
nuROCRPredictions <- ROCR::prediction(nuProbabilities, test$survival, label.ordering = nuInverse)
nuPerformance <- ROCR::performance(nuROCRPredictions, "tpr", "fpr")
```

::: panel-tabset
### ROC Curve with AUC

```{r}
#| label: primary-model-roc-auc
#| echo: false 
plot(nuPerformance, colorize = FALSE, main="ROC Curve for nu-classification")
abline(a=0,b=1)
mtext(paste("AUC =",round(as.numeric(nuROC$auc), 4), " F1 Score = ", round(as.numeric( nuConfusionMatrix$byClass["F1"]),4)))
```

### Confusion Matrix

```{r}
#| label: primary-model-confusion-matrix
#| echo: false
fourfoldplot(as.table(nuConfusionMatrix), color=c("navy", "lightblue"), main="nu-classification Confusion Matrix")
```
:::

## Model Results (Challenger)

```{r}
#| label: nu-model-challenger

set.seed(params$final_rand_seed)
challnger_test <- test %>% select(sysbp_mean, resprate_mean, tempc_mean, platelet_min, is_male, age_range, heart_rate, survival, icustay_id)
challnger_train <- train %>% select(sysbp_mean, resprate_mean, tempc_mean, platelet_min, is_male, age_range, heart_rate, survival, icustay_id)
nuChallengerModel <- e1071::svm(survival ~ . - icustay_id,
                      type="nu-classification",
                      kernel="radial",
                      data=challnger_train,
                      probability=TRUE,
                      gamma=params$nu_challenger_gamma,
                      nu=params$nu_challenger_nu,
                      scale=TRUE)

nuChallengerPredictions <- predict(nuChallengerModel, challnger_test, probability=TRUE)
nuChallengerProbabilities <- attr(nuChallengerPredictions, "probabilities")[,1]
nuChallengerROC <- pROC::roc(as.factor(challnger_test$survival), nuChallengerProbabilities)
nuChallengerConfusionMatrix <- confusionMatrix(nuChallengerPredictions, challnger_test$survival, mode="everything", positive="SURVIVED")

#to appropriately build the roc graph, buid a label order based on survivalLevels and the positive case from the confusion matrix


nuChallengerOrdering <- c( nuConfusionMatrix$positive, survivalLevels[which(survivalLevels != nuConfusionMatrix$positive)])

nuChallengerROCRPredictions <- ROCR::prediction(nuChallengerProbabilities, challnger_test$survival, label.ordering = nuChallengerOrdering)
nuChallengerPerformance <- ROCR::performance(nuChallengerROCRPredictions, "tpr", "fpr")


```

::: panel-tabset
### ROC Curve with AUC

```{r}
#| label: challenger-model-roc-auc
#| echo: false
plot(nuChallengerPerformance, colorize = FALSE, main="ROC Curve for nu-classification (Challenger)")
abline(a=0,b=1)
mtext(paste("AUC =",round(as.numeric(nuChallengerROC$auc), 4), " F1 Score = ", round(as.numeric( nuChallengerConfusionMatrix$byClass["F1"]),4)))

```

### Confusion Matrix

```{r}
#| label: challenger-model-confusion-matrix
#| echo: false
fourfoldplot(as.table(nuChallengerConfusionMatrix), color=c("navy", "lightblue"), main="nu-classification Challenger Confusion Matrix")
```
:::

# Results and Conclusions

With Brad Lipson

## Data

```{r}
#| label: data-set-table
#| echo: false

library(gtsummary)

data_for_display <- mimic_data %>%
mutate(
  gender = case_when(gender == "M" ~ "male",
                     gender == "F" ~ "female",
                     TRUE ~ gender),
  survived = case_when(hospital_expire_flag == 1 ~ "Died",
                     hospital_expire_flag == 0 ~ "Survived")
) %>%  
select(
  age, 
  gender, 
  survived, 
  heartrate_mean,
  sysbp_mean,
  resprate_mean,
  tempc_mean,
  wbc_mean,
  platelet_min,
  creatinine_max,
  lactate_mean
) 
```

```{r}
#| label: data-set-table-first
#| echo: false

  data_for_display %>% 
  select(age, gender, heartrate_mean, sysbp_mean, survived)  %>% 
  tbl_summary(
    type = list(age ~ 'continuous2',
    gender ~ 'categorical',
    heartrate_mean ~ 'continuous2',
    sysbp_mean ~ 'continuous2'),
    label = list(
      age ~ "Patient Age",
      gender ~ "Patient Sex",
      heartrate_mean ~ "Heart Rate",
      sysbp_mean ~ "Systolic Blood Pressure"
       ),
      statistic = all_continuous() ~ c("{median}({p25}, {p75})",
      "{min}, {max}"),
      by = survived 
  ) %>%
  add_overall(last = TRUE) %>%
  bold_labels() %>%
  italicize_levels() %>%   as_gt() %>%
  gt_theme_dark() %>%
  tab_options(
    table.background.color = "#d8e4ea",
    column_labels.background.color="#5092c2",
    table.align = "left"
  )
```

## Data Continued

```{r}
#| label: data-set-table-second
#| echo: false

data_for_display %>% 
  select(resprate_mean, tempc_mean, wbc_mean, survived)  %>% 
    tbl_summary(
    type = list(resprate_mean ~ 'continuous2',
    tempc_mean ~ 'continuous2',
    wbc_mean ~ 'continuous2'
    ),
    label = list(
      resprate_mean ~ "Respiration Rate",
      tempc_mean ~ "Body Temperature (c)",
      wbc_mean ~ "White Blood Cell Count"
       ),
      statistic = all_continuous() ~ c("{median}({p25}, {p75})",
      "{min}, {max}"),
      by = survived 
  ) %>%
  add_overall(last = TRUE) %>%
  bold_labels() %>%
  italicize_levels() %>%   as_gt() %>%
  gt_theme_dark() %>%
  tab_options(
    table.background.color = "#d8e4ea",
    column_labels.background.color="#5092c2",
    table.align = "left"
  )
```

## Data Continued again

```{r}
#| label: data-set-table-three
#| echo: false
  data_for_display %>% 
  select(platelet_min,creatinine_max,lactate_mean, survived)  %>% 
    tbl_summary(
    type = list(
    platelet_min ~ 'continuous2',
    creatinine_max ~ 'continuous2',
    lactate_mean ~ 'continuous2'
    ),
    label = list(
      platelet_min ~"Platelet Count",
      creatinine_max ~"Creatinine Level",
      lactate_mean ~"Lactate Level"
       ),
      statistic = all_continuous() ~ c("{median}({p25}, {p75})",
      "{min}, {max}"),
      by = survived 
  ) %>%
  add_overall(last = TRUE) %>%
  bold_labels() %>%
  italicize_levels() %>%   as_gt() %>%
  gt_theme_dark() %>%
  tab_options(
    table.background.color = "#d8e4ea",
    column_labels.background.color="#5092c2",
    table.align = "left"
  )

```

## Results

-   The findings indicate that our support vector machine (SVM) model had a test set accuracy of 74.74%.
-   The model exhibited a sensitivity rate of 74.87% and a specificity rate of 63.89%
-   The study yielded a positive predictive value of 99.41% and a negative predictive value of 3%.
-   The area under the receiver operating characteristic (ROC) curve was determined to be 0.7711
-   The F1 score was determined to be 0.8541.

## Model Conclusions

-   Our support vector machine (SVM) model demonstrated moderate accuracy in predicting hospital mortality.
-   The predictive accuracy of the model was higher for patients who survived compared to those who did not.
-   The model can tell the difference between patients who will die and those who will not.

## Considerations for improvement

-   Potential to serve as a valuable tool for forecasting patient mortality in hospital settings.
-   Additional investigation is required to substantiate these results among a broader and more heterogeneous sample.
-   Constraints since the investigation was carried out on a limited cohort of individuals.
-   Data from just one hospital, should include globally

## Considerations for improvement (Cont'd)

-   Should improve diversity in future studies
-   Limit the applicability of the findings to other healthcare institutions.
-   Failed to account for other potential confounders of the ICU patients.

## Future Studies

-   Examine the outcomes of this study in a bigger, more heterogeneous sample.
-   Further investigation is warranted to explore the application of SVMs in predicting additional clinical outcomes
-   Should study duration of hospitalization and rates of patient readmission
-   Predict length of stay in hospital
-   Predict length of time patients may live with certain conditions, depending on severity

## Thank you! Any Questions?
