[
  {
    "objectID": "slides.html#important-concepts",
    "href": "slides.html#important-concepts",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nused the e1071 r package\nleveraged the tune(svm, ...) function for tuning our model hyperparameters\n\n\nHowever before we go much further lets get some basic terminology out of the way and outline some technological decisions we made.\nFirst we selected the e1071 r package to build, train, and test our model. There are alternatives out there however e1071 appears to be the most popular\nSecondly we didn’t really know what the best hyperparameters or even kernel where so we leveraged the tune method to try a bunch of tunings to see what produced the best accuracy"
  },
  {
    "objectID": "slides.html#our-intial-primary-model",
    "href": "slides.html#our-intial-primary-model",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Our intial primary model",
    "text": "Our intial primary model\n\nwas of type “C-Classification”\nleveraged a linear kernel\nonly tuned the cost hyperparameter\n\n\nwe selected C-classification which is the default because it selects the response variable as a binary classifier we also tested a linear kernel because nothing jumped out at us when doing data exploration that our data would not be easily linearly seperable we tuned cost which is the penalty for an incorrect prediction. we did this by passing a list of cost values from 0.001 to 100 with each step being a factor of 10"
  },
  {
    "objectID": "slides.html#initial-model-analysis",
    "href": "slides.html#initial-model-analysis",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Initial Model Analysis",
    "text": "Initial Model Analysis\n\nROC Cuve with AUCConfusion Matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn our ROC Curve we can see that our model has roughly a 74% accuracy as that is the area under the ROC Curve. Ideally that number would be higher but it was acceptable as anything above the 50% diagonal line means that our model is more accurate than flipping a coin\n\nOur confusion matrix however was a disaster. As you can see, despite having an acceptable accuracy, our model was very optimistic and predicted that everyone who entered ICU would survive. While we would love for this to match reality, we know its not true. Additionally a model that assumes everyone survives is of little value given"
  },
  {
    "objectID": "slides.html#fixing-the-model",
    "href": "slides.html#fixing-the-model",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Fixing the model",
    "text": "Fixing the model\nWe attempted 2 strategies to fix the\n\nOversampling of minority case\n\nEnsure that minority case was a larger percentage of the training set via selection with replacement\n\nDownsampling majority case\n\nTake a large percentage of the minority cases, select and EQUIVALENT number of majority cases for a 50/50 split\n\n\n\nFirst we attempted to oversample our minority case by selecting ensuring that they met a certain precentage of the train data set and allowed random sampling to occur with replacement (meaning we could select the same observation multiple times) This improved the model but not significantly Secondly we downsampled the majority case by selecting 95% of the minority case, then randomly selecting and EQUIVALENT number of the majority cases for a 50/50 split This proved better than oversampling, but still wasnt optimal"
  },
  {
    "objectID": "slides.html#tuning-the-new-model",
    "href": "slides.html#tuning-the-new-model",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Tuning the new model",
    "text": "Tuning the new model\n\ntested using a rbf (radial bias kernel)\nswitched to nu-classification\ntuned for nu and gamma\n\nusing grid search strategy and a bunch of packages\n\n\n\nwhen we tuned we decided to try a few other options of the e1071 package. First we switched to a radial basis kernel. R secondly we switched to nu classification. Which according the documentation is the same as C classification but it constrains the nu value between 0 and 1, nu its self is related to the ratio of support vectors and the ratio of the training error we now had to tune for nu and gamma hyperparemeters we had no clue in how to do this and since our model took a large hit in accuracy by downsampling we wanted to try a bunch of things and select the best to do this we leveraged a grid search strategy which is effectively a brute force attemp at finding hyperparameters"
  },
  {
    "objectID": "slides.html#tuning-the-new-modelmore-problems",
    "href": "slides.html#tuning-the-new-modelmore-problems",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Tuning the new model…more problems",
    "text": "Tuning the new model…more problems\n\nTune SVM is VERY VERY SLOW\n\ndoes not leverage mulitiple cpu cores\noperates sequentially\n\ndoParallel and foreach to the rescue\nstill slow but much quicker at the same time\n\n\ngiven we had two hyperparameters to tune and wanted the best accuracy we attempted to use grid search with tune svm. I set this up on my machine, walked away, came back a few hours later and nothing had completed. eventually my machine crashed Discovered do parallel and foreach to allow me to pass the variables in and build and validate models, aggregate results and pick the best parameters"
  },
  {
    "objectID": "slides.html#tuning-the-new-model-and-paying-more-for-it",
    "href": "slides.html#tuning-the-new-model-and-paying-more-for-it",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Tuning the new model and paying more for it",
    "text": "Tuning the new model and paying more for it\n\n\nThis is a fun graph to show the out comes, this is on a an 8 core 10th gen intel cpu. Training took around 45 minutes for the nu classification and 20 minutes c-classification\nMy CPU ran near 96 degrees celsius @ 4.8 ghz for the duration of the test"
  },
  {
    "objectID": "slides.html#tuning-results",
    "href": "slides.html#tuning-results",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Tuning Results",
    "text": "Tuning Results\n\nnu-classification won out\nPrimary Model\n\nGamma = 0.02\nNu = 0.75\n\nChallenger model\n\nGamma = 0.17\nNu = 0.87"
  },
  {
    "objectID": "slides.html#model-results-primary",
    "href": "slides.html#model-results-primary",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Model results (Primary)",
    "text": "Model results (Primary)\n\nROC Curve with AUCConfusion Matrix"
  },
  {
    "objectID": "slides.html#model-results-challenger",
    "href": "slides.html#model-results-challenger",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Model Results (Challenger)",
    "text": "Model Results (Challenger)\n\nROC Curve with AUCConfusion Matrix"
  },
  {
    "objectID": "slides.html#data",
    "href": "slides.html#data",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Died, N = 7411\n      Survived, N = 3,8181\n      Overall, N = 4,5591\n    \n  \n  \n    Patient Age\n\n\n\n        Median(IQR)\n73(60, 83)\n65(53, 78)\n67(54, 80)\n        Range\n17, 91\n17, 91\n17, 91\n    Patient Sex\n\n\n\n        female\n339 (46%)\n1,639 (43%)\n1,978 (43%)\n        male\n402 (54%)\n2,179 (57%)\n2,581 (57%)\n    Heart Rate\n\n\n\n        Median(IQR)\n92(77, 106)\n87(76, 99)\n88(76, 100)\n        Range\n47, 155\n36, 139\n36, 155\n    Systolic Blood Pressure\n\n\n\n        Median(IQR)\n108(100, 120)\n115(106, 126)\n114(105, 126)\n        Range\n70, 175\n76, 195\n70, 195\n        Unknown\n2\n6\n8\n  \n  \n  \n    \n      1 n (%)"
  },
  {
    "objectID": "slides.html#data-continued",
    "href": "slides.html#data-continued",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Data Continued",
    "text": "Data Continued\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Died, N = 741\n      Survived, N = 3,818\n      Overall, N = 4,559\n    \n  \n  \n    Respiration Rate\n\n\n\n        Median(IQR)\n21.5(18.4, 24.9)\n18.9(16.6, 21.9)\n19.3(16.8, 22.4)\n        Range\n11.3, 40.6\n9.5, 40.4\n9.5, 40.6\n        Unknown\n0\n1\n1\n    Body Temperature (c)\n\n\n\n        Median(IQR)\n36.62(36.11, 37.19)\n36.87(36.47, 37.32)\n36.82(36.41, 37.31)\n        Range\n31.60, 39.71\n32.61, 40.10\n31.60, 40.10\n        Unknown\n20\n83\n103\n    White Blood Cell Count\n\n\n\n        Median(IQR)\n13(9, 18)\n12(8, 15)\n12(8, 16)\n        Range\n0, 404\n0, 207\n0, 404"
  },
  {
    "objectID": "slides.html#data-continued-again",
    "href": "slides.html#data-continued-again",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Data Continued again",
    "text": "Data Continued again\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Died, N = 741\n      Survived, N = 3,818\n      Overall, N = 4,559\n    \n  \n  \n    Platelet Count\n\n\n\n        Median(IQR)\n166(95, 253)\n180(126, 245)\n178(122, 246)\n        Range\n8, 951\n5, 1,297\n5, 1,297\n        Unknown\n1\n5\n6\n    Creatinine Level\n\n\n\n        Median(IQR)\n1.60(1.00, 2.60)\n1.10(0.80, 1.70)\n1.20(0.90, 1.90)\n        Range\n0.20, 14.40\n0.10, 27.80\n0.10, 27.80\n        Unknown\n1\n1\n2\n    Lactate Level\n\n\n\n        Median(IQR)\n2.55(1.70, 4.50)\n1.80(1.30, 2.55)\n1.90(1.35, 2.75)\n        Range\n0.40, 20.85\n0.30, 16.80\n0.30, 20.85"
  },
  {
    "objectID": "slides.html#results",
    "href": "slides.html#results",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Results",
    "text": "Results\n\nThe findings indicate that our support vector machine (SVM) model had a test set accuracy of 74.74%.\nThe model exhibited a sensitivity rate of 74.87% and a specificity rate of 63.89%\nThe study yielded a positive predictive value of 99.41% and a negative predictive value of 3%.\nThe area under the receiver operating characteristic (ROC) curve was determined to be 0.7711\nThe F1 score was determined to be 0.8541."
  },
  {
    "objectID": "slides.html#model-conclusions",
    "href": "slides.html#model-conclusions",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Model Conclusions",
    "text": "Model Conclusions\n\nOur support vector machine (SVM) model demonstrated moderate accuracy in predicting hospital mortality.\nThe predictive accuracy of the model was higher for patients who survived compared to those who did not.\nThe model can tell the difference between patients who will die and those who will not."
  },
  {
    "objectID": "slides.html#considerations-for-improvement",
    "href": "slides.html#considerations-for-improvement",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Considerations for improvement",
    "text": "Considerations for improvement\n\nPotential to serve as a valuable tool for forecasting patient mortality in hospital settings.\nAdditional investigation is required to substantiate these results among a broader and more heterogeneous sample.\nConstraints since the investigation was carried out on a limited cohort of individuals.\nData from just one hospital, should include globally"
  },
  {
    "objectID": "slides.html#considerations-for-improvement-contd",
    "href": "slides.html#considerations-for-improvement-contd",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Considerations for improvement (Cont’d)",
    "text": "Considerations for improvement (Cont’d)\n\nShould improve diversity in future studies\nLimit the applicability of the findings to other healthcare institutions.\nFailed to account for other potential confounders of the ICU patients."
  },
  {
    "objectID": "slides.html#future-studies",
    "href": "slides.html#future-studies",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Future Studies",
    "text": "Future Studies\n\nExamine the outcomes of this study in a bigger, more heterogeneous sample.\nFurther investigation is warranted to explore the application of SVMs in predicting additional clinical outcomes\nShould study duration of hospitalization and rates of patient readmission\nPredict length of stay in hospital\nPredict length of time patients may live with certain conditions, depending on severity"
  },
  {
    "objectID": "slides.html#thank-you-any-questions",
    "href": "slides.html#thank-you-any-questions",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Thank you! Any Questions?",
    "text": "Thank you! Any Questions?"
  },
  {
    "objectID": "about.html#warnings-and-caveats",
    "href": "about.html#warnings-and-caveats",
    "title": "Predicting Survival of Intensive Care Unit Paitents with Support Vector Machines",
    "section": "Warnings and Caveats",
    "text": "Warnings and Caveats\nWe used easy packages to install packages. as part of this it will run the install from everything in pkgs.r. As such, running a render or a preview will attempt to install any packages without prompting you to agree to installing them. We have hardcoded the repo for cran in the package installer but if thats compremised the auto install could cause security concerns."
  },
  {
    "objectID": "about.html#rendering-the-page",
    "href": "about.html#rendering-the-page",
    "title": "Predicting Survival of Intensive Care Unit Paitents with Support Vector Machines",
    "section": "Rendering the page",
    "text": "Rendering the page\nFor best results, run quarto render --cache as this takes a long time to render the visualizations and build and tune the model.\nYou will need to install the following system libraries in order for the r code to work properly\n\nlibgit2\nlibharfbuzz-dev (debian, ubuntu, etc)\n\nharfbuzz-devel(fedora, EPEL)\n\nlibfribidi-dev (debian, ubuntu, etc)\n\nfribidi-devl (fedora, EPEL) libv8 or libnode-dev\n\n\nsudo apt-get install libcurl4-openssl-dev\nsudo apt-get install libcurl4-openssl-dev sudo apt-get install libcurl4-openssl-dev # Complete References ::: {#refs} :::"
  },
  {
    "objectID": "Journal/josh.html",
    "href": "Journal/josh.html",
    "title": "Josh’s Journal Entries",
    "section": "",
    "text": "In Machine learning in medicine: a practical introduction the authors provide a introduction to machine learning in the medical field and a survey of 3 supervised learning methodologies. The begin by explaining how machine learning is related to traditional statistical inference but noted that a major difference is that statistical inference aims to “reach conclusions about a population…” (Sidey-Gibbons, 2) while machine learning aims to predict a specific out come. The authors go on to state that the use of ML in the medical field is relatively easy to explain because many features or parameters that are input can be reasoned about when the prediction is made. An example the authors used were “body mass index and diabetes risk” as the linkage between the two are relatively well known and understood. The author’s then go on to explain the difference between Auditable Algorithms that are easily understood and black box algorithms which are complex and can be hard to reason about. Support Vector Machines can be a member of the later group but not always.\nThe authors then leverage a Generalized Linear model, Support Vector Machine, and Artificial Neural Network to predict whether or not a given breast tissue sample is cancerous. I will skip over the ANN and GLM as our group is not directly focused on them. However with the Support Vector Machine implementation they authors did note that the overall goal is to build a hyperplane that separates two categories the best. Some datasets do not easily separate so it is possible to rearrange the date by using a kernel trick or kernel function to increase the amount of linear separation between observations.\nFinally the authors compared the outcomes of their three algorithms and found that the best one (in terms of accuracy) for the data set they had was the support vector machine. The way they determined this was to build a ROC Curve to find the number of true true predictions and true false predictions\nUWF Access Url\n\n\n\nIn Dibiki et al’s paper on Support vector machines he discussed what SVM’s are and what that the underlying statistical methods are. The paper overall was heavily math based and at times over my head. However it did have information about how and why support vector machines are built. Predominately the author notes that reseacher Vapnik constructed SVM’s based off of Structural Risk Minimaization(SRM) as opposed to Empirical Risk minimization and that SRM proved to be better at creating more generalizable models.\nThe author goes on to explain that the hyperplane is a the maximal margin between data points in a data set. This is called the “Optimal Separating Hyperplane” The authors then went through the math of how to solve for the problems and examples of how to chose kernel functions to ensure that a hyperplane can be found.\nThe examples the author used was were the classification if image data to determine classification of land cover (water, forest, roads, etc) Another example was comparing the performance of an Artificial Neural Network to Support Vector machines to predict stream flow data based on daily rainfall and evaporation based on 3 different locations. The general finding was that SVM and ANN’s can both be readily leveraged to build similar models and predictions\nUWF Access Url"
  },
  {
    "objectID": "Journal/josh.html#machine-learning-in-medicine-a-practical-introduction.-gibbons2019",
    "href": "Journal/josh.html#machine-learning-in-medicine-a-practical-introduction.-gibbons2019",
    "title": "Josh’s Journal Entries",
    "section": "",
    "text": "In Machine learning in medicine: a practical introduction the authors provide a introduction to machine learning in the medical field and a survey of 3 supervised learning methodologies. The begin by explaining how machine learning is related to traditional statistical inference but noted that a major difference is that statistical inference aims to “reach conclusions about a population…” (Sidey-Gibbons, 2) while machine learning aims to predict a specific out come. The authors go on to state that the use of ML in the medical field is relatively easy to explain because many features or parameters that are input can be reasoned about when the prediction is made. An example the authors used were “body mass index and diabetes risk” as the linkage between the two are relatively well known and understood. The author’s then go on to explain the difference between Auditable Algorithms that are easily understood and black box algorithms which are complex and can be hard to reason about. Support Vector Machines can be a member of the later group but not always.\nThe authors then leverage a Generalized Linear model, Support Vector Machine, and Artificial Neural Network to predict whether or not a given breast tissue sample is cancerous. I will skip over the ANN and GLM as our group is not directly focused on them. However with the Support Vector Machine implementation they authors did note that the overall goal is to build a hyperplane that separates two categories the best. Some datasets do not easily separate so it is possible to rearrange the date by using a kernel trick or kernel function to increase the amount of linear separation between observations.\nFinally the authors compared the outcomes of their three algorithms and found that the best one (in terms of accuracy) for the data set they had was the support vector machine. The way they determined this was to build a ROC Curve to find the number of true true predictions and true false predictions\nUWF Access Url"
  },
  {
    "objectID": "Journal/josh.html#model-induction-with-support-vector-machines-introductions-and-applications.dibike2001",
    "href": "Journal/josh.html#model-induction-with-support-vector-machines-introductions-and-applications.dibike2001",
    "title": "Josh’s Journal Entries",
    "section": "",
    "text": "In Dibiki et al’s paper on Support vector machines he discussed what SVM’s are and what that the underlying statistical methods are. The paper overall was heavily math based and at times over my head. However it did have information about how and why support vector machines are built. Predominately the author notes that reseacher Vapnik constructed SVM’s based off of Structural Risk Minimaization(SRM) as opposed to Empirical Risk minimization and that SRM proved to be better at creating more generalizable models.\nThe author goes on to explain that the hyperplane is a the maximal margin between data points in a data set. This is called the “Optimal Separating Hyperplane” The authors then went through the math of how to solve for the problems and examples of how to chose kernel functions to ensure that a hyperplane can be found.\nThe examples the author used was were the classification if image data to determine classification of land cover (water, forest, roads, etc) Another example was comparing the performance of an Artificial Neural Network to Support Vector machines to predict stream flow data based on daily rainfall and evaporation based on 3 different locations. The general finding was that SVM and ANN’s can both be readily leveraged to build similar models and predictions\nUWF Access Url"
  },
  {
    "objectID": "Journal/josh.html#data-mining-concepts-and-techniques-han2012",
    "href": "Journal/josh.html#data-mining-concepts-and-techniques-han2012",
    "title": "Josh’s Journal Entries",
    "section": "Data Mining: Concepts and Techniques (Han and Pei 2012)",
    "text": "Data Mining: Concepts and Techniques (Han and Pei 2012)\nThis review of the text book contains more general information about support vector machines and various applications. It defines that the maximal marginal hyperplane is the single line that can be drawn between two “clusters” of data.This MMH (maximal marginal hyperplane) is defined as a line that can be drawn where the distances between two clusters of data is the largest. It also explains that while we can think of it as a line, the MMH is an actual plane that can support more than 2 dimensions. Additionally the vectors where the hyperplane touch are called the support vectors as the effectively define the sides of the hyper plane.\nIn general the book explains that Support vector machines work very well with linear data sets, their use of kernel functions allow them to operate on non linear data. Kernel functions according to the book can be thought of as mathematic tricks that transform/map data from one dimension to a higher dimensions that is linearly separable."
  },
  {
    "objectID": "Journal/josh.html#support-vector-machines-and-kernel-methods-the-new-generation-of-learning-machines-cristianini2002",
    "href": "Journal/josh.html#support-vector-machines-and-kernel-methods-the-new-generation-of-learning-machines-cristianini2002",
    "title": "Josh’s Journal Entries",
    "section": "Support vector machines and kernel methods: the new generation of learning machines (Cristianini and Scholkopf Fall 2002)",
    "text": "Support vector machines and kernel methods: the new generation of learning machines (Cristianini and Scholkopf Fall 2002)\nIn the article, the author briefly discusses the history of machine learning and its evolution from working predominately on linearly separable datasets to the advancements made with handling non linear data in the 1980s. The author then goes on to discuss the work and presentations of Vapnik et al in 1992. This allowed those wanting to do machine learning data on non linear data in a “principled yet efficient manner”.  (Cristianini and Scholkopf)\none of the other key points that the other makes is that the higher the dimensionality of the problems space, the harder it is to create predictions based on it.\nThe other new and interesting points pointed by the authors where that SVM’s hold records (at the time the article was written) for ability to read handwritten digits and other tasks.This leads to it being very well suited for hand writing detection. Other areas that the models are good according to the author are “text categorization, handwritten digit recognition, and gene expression data classification” (Cristianini and Scholkopf)\nUWF Access Url"
  },
  {
    "objectID": "Journal/josh.html#fast-training-of-support-vector-machines-for-survival-analysis-polsterl2015",
    "href": "Journal/josh.html#fast-training-of-support-vector-machines-for-survival-analysis-polsterl2015",
    "title": "Josh’s Journal Entries",
    "section": "Fast Training of Support Vector Machines for Survival Analysis (Pölsterl, Navab, and Katouzian 2015)",
    "text": "Fast Training of Support Vector Machines for Survival Analysis (Pölsterl, Navab, and Katouzian 2015)\nIn Fast Training of Support Vector Machines for Survival Analysis the author explains that they wish to look at 3 different methods of training a support vector machine for survival analysis: ranking, regression, and a combination of ranking and regressions to determine how well they predict survivability. The author also introduces the concept of censored data. This is a term i hadn’t heard of before but makes sense when explained and in the context of survivability prediction. As explained by the author, data is uncensored if a significant event occurs during the time period in which the study or model is used for. Meanwhile censored data is data in which the event did not occur during the study or observation period, however it may have occurred after the study completes. When thinking about patient survivability this makes sense. For example if we want to study whether or not a patient dies during a hospital stay we probably only want to predict that for a fixed time period (the hospital stay) as we all know every patient will eventually die…perhaps even in a (different) hospital stay!\nThe author then goes on to show how ranking methods such as Cox proportional Hazards and others fair when given certain sized data sets and then comparing that with regression based model. The author summarizes at the end that using ordered statistic trees (which their algorithm uses) is a sufficiently accurate and fast model for predicting patient survivability."
  },
  {
    "objectID": "Journal/josh.html#mortality-prediction-based-on-imbalanced-high-dimensional-icu-big-data-liu2018",
    "href": "Journal/josh.html#mortality-prediction-based-on-imbalanced-high-dimensional-icu-big-data-liu2018",
    "title": "Josh’s Journal Entries",
    "section": "Mortality prediction based on imbalanced high-dimensional ICU big Data (Liu June 2018)",
    "text": "Mortality prediction based on imbalanced high-dimensional ICU big Data (Liu June 2018)\nMortality prediction based on imbalanced high-dimensional ICU big data takes a look at predicting mortality based on a large number of data dimensions with various amounts of data missing. Over all this paper appears to follow an approach that would be good for our project using the MIMIC data set.\nMost of the article goes beyond the scope of Support Vector Machines but delves into principal component analysis to determine what to use to build the support vector machines.The author leverages Cost Sensitive Principal Component Analysis to preprocess the data to deal with missing data and feature extraction. Once this preprocessing step has completed, the authors build a support vector machine to predict mortality. The also build a number of other support vector machines using Chaos particle swarm optimization for parameter optimization and derivatives of CPSO to determine the best model based on the ROC AUC value. In the end the found the SVM using data that had been processed with their modified Cost Sensitive Principal Component Analysis and SPSO\nThrough their findings the authors also opine about the large amount of data and the necessity of determining which are the key features to from which to build a model such as a SVM. They state that the overall number of data points will continue to increase as sensors and technology are continually introduced and improved upon in medical settings and that while the data is great and represents a virtual gold mine, it is important to ensure that data is clean and useful for prediction and not just noisy data for algorithms to churn through"
  },
  {
    "objectID": "Journal/josh.html#artificial-intelligence-in-the-intensive-care-unit-greco2020",
    "href": "Journal/josh.html#artificial-intelligence-in-the-intensive-care-unit-greco2020",
    "title": "Josh’s Journal Entries",
    "section": "Artificial Intelligence in the Intensive Care Unit (Greco, Caruso, and Cecconi 2020)",
    "text": "Artificial Intelligence in the Intensive Care Unit (Greco, Caruso, and Cecconi 2020)\nThe authors of Artificial Intelligence in the Intensive Care Unit describe the ways that medicine can benefit by the usage of machine learning and compares various methods for machine learning. The author goes in depth about why Intensive Care Units are a great place for the introduction of big data practices and machine learning in hospital settings. After introducing the methods of machine learning the paper then discusses the limits of machine learning, examples of machine learning in critical care and the future of big data and machine learning in medicine.\nLive Access URL"
  },
  {
    "objectID": "Journal/josh.html#predictive-modelling-of-survival-and-length-of-stay-in-critically-ill-patients-using-sequential-organ-failure-scores-houthooft2015",
    "href": "Journal/josh.html#predictive-modelling-of-survival-and-length-of-stay-in-critically-ill-patients-using-sequential-organ-failure-scores-houthooft2015",
    "title": "Josh’s Journal Entries",
    "section": "Predictive modelling of survival and length of stay in critically ill patients using sequential organ failure scores (Houthooft et al. 2015)",
    "text": "Predictive modelling of survival and length of stay in critically ill patients using sequential organ failure scores (Houthooft et al. 2015)\nThe paper “Predictive modelling of survival and length of stay in critically ill patients using sequential organ failure scores” the authors talk about ways to use machine learning to model the length of stay as a predictor of patient mortality. The author talks about choosing the data sets and selecting certain features for modeling based on data from th first five days of a patients stay to predict both the patients mortality and their length of stay. The results of the SVM the author built were compared to regression results to model the length of stay and then compared with mortality for patients with lengthy ICE stays. The author goes on to conclude that the models can be helpful to support physicians allocate ICU resources and make decisions during a patients time in ICU."
  },
  {
    "objectID": "Journal/brad.html",
    "href": "Journal/brad.html",
    "title": "SVM application in Data Mining in EMR",
    "section": "",
    "text": "Support vector machines (SVMs) are a powerful method for machine learning that can be used for data mining. There are several different SVM kernels, and it is not always clear which one is best for a certain job. The goal of this paper is to help data scientists pick the best SVM kernel for a given job. The authors looked at how well different SVM models did at classification, regression, and clustering, among other data mining tasks. They used both real-world data and data that they made up themselves. The article by Xu et al. aimed to see how well various SVM kernels did at data mining jobs. They found that SVM with the RBF kernel did the best job at most data mining tasks. However, they also found that the performance of the different SVM kernels relies on the task and data set. One problem with this study is that there were only a few data mining jobs carried out. (5)\nMy next journal suggested a new SVM algorithm for jobs related to data mining. This is important since SVM is a powerful machine learning method, but they can be hard to train, especially on big datasets. The goal of this study is to suggest a new SVM algorithm that works better for data mining. They came up with a new SV algorithm that is made for data mining jobs. The program uses several methods to improve how well SVM training works. They tested how well their new SVM algorithm did at classification, regression, and grouping, among other tasks in data mining. They found that their new SVM algorithm was better at most data mining jobs than other SVM algorithms. But this algorithm has a weakness in that it is harder to understand than other SVM algorithms. (3)\nIn addition, Zhou et al wrote about deep mining of electronic medical data using support vector machines to predict the prognosis of severe, acute myocardial infarction. The authors talked about how the MIMIC-3 database is used to find the 13 markers for heart attack cases. They compared SVM algorithms and found that the model was about 92% accurate. They use this model to pull out certain features from the EMR and identify which patients will have a MI. They said that this helps doctors figure out the classification regression parts of a disease outlook. (6)\nMy next piece was about how Fouodo et al and others used support vector machines for survival analysis with R. They used the survivalSVM package to do three different kinds of survival analysis. They used both regression and ranking, which is a mix of the two. The next way to find the constraints was to use regression followed by Cox proportional hazard models. They stated that the SVM worked about as well as other methods on the datasets they used. So, this R package makes it quick and easy to find out how likely a patient is to live. (2)\nAnother article was called “Using Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records.” The goal of this work is to show how support vector machines (SVMs) in electronic medical records (EMRs) can be used to classify diabetes mellitus. This study looked at how well SVMs can classify diabetes because they have been good at diagnosing other diseases from electronic medical records (EMRs). The writers used EMRs from both people with and without diabetes to train an SVM model. During preparation, noise and outliers were first taken out of the EMRs. The SVM model was then trained with the help of guided learning. (1)\nThe next journal discussed a way to predict hospital readmissions using support vector machines. The goal of this study is to make a support vector machine (SVM) model that can predict a patient's return to the hospital. The importance was that going back to the hospital is a deadly problem in health care, and it can be expensive for patients. A reliable predictor of hospital readmission could help hospitals find people who are at risk and give them treatment to keep them from going back to the hospital. A solution is that a collection of electronic medical records (EMRs) was used to train an SVM model. First, during preprocessing, abnormalities were taken out of the EMRs. The SVM model was then trained with the help of guided learning and separated the information into two groups. With the SVM model, this included readmitted patients who had to go back to the hospital. (4)\nSVM was used by Vieira et al. to divide data into two groups. The algorithm maps the raw data to a high-dimensional feature space, where a linear classification surface is made. The SVM method then tries to find the best hyperplane that separates the two types of data by the most. The margin is the distance between the hyperplane and the data points in each group that are closest to the hyperplane. The SVM algorithm also uses a kernel function to move the raw data into a space with more dimensions, where it is easier to separate. The kernel function is a piece of math that figures out how similar two data points are to each other. SVM also uses regularization to control the trade-off between making the margin as big as possible and making the classification mistake as small as possible. The SVM algorithm learns from a set of labeled data, where each data point has a label that tells what group it belongs to. Once the SVM algorithm has been taught, it can be used to put new data points that have not been labeled into one of the two groups. (7)\nYang et al. evaluated the performance a version of GAN called conditional medical GAN (C-med GAN) could determine who would die among ICU patients. The study used data from the Medical Information Mart for Intensive Care III (MIMIC-III) database and compared the success of the C-med GAN with some baseline models, such as the simplified acute physiology score II (SAPS II), the support vector machine (SVM), and the multilayer perceptron (MLP). The dataset was split into three sizes, and a 5-fold grid search cross-validation process was used to find the best hyperparameters and then the best model selection for the C-med GAN. Area under the precision-recall curve (PR-AUC), area under the receiver operating characteristic curve (ROC-AUC), and F1 score were used to measure the C-med GAN’s accuracy. The study came up with a helpful method to use SAPS II results to directly estimate how long a patient will live. The results of this study could be used in intensive care to make it easier to predict mortality in the ICU. (8)\nReferences\n(1) Adeoye, Abiodun O., et al. Utilizing Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records. International Journal of Advanced Computer Science and Information Technology (IJACSIT), vol. 11, no. 10, 2021, pp. 102-114.\n(2) Fouodo, Cesaire, et al. Support Vector Machines for Survival Analysis with R. R Journal, vol. 14, no. 2, 2022, pp. 92-107.\n(3) Hu, Xiangfen, Wei Huang, and Qiang Wu. A New Support Vector Machine Algorithm for Data Mining.\" Knowledge-Based Systems, vol. 112, 2016, pp. 118-128.\n(4) Ismail, Gaber A., et al. An Approach Using Support Vector Machines to Predict Hospital Readmission.\" Journal of Medical Systems, vol. 44, no. 9, 2020, pp. 1-10.\n(5) Xu, Fei, Lihong Li, and Zhihua Zhou. SVM Kernels for Data Mining: A Comparative Study.\" Proceedings of the 2010 SIAM International Conference on Data Mining (SDM), 2010, 585-596.\n(6) Zhou, Xingyu, et al. Using Support Vector Machines for Deep Mining of Electronic Medical Records in Order to Predict Prognosis of Severe, Acute Myocardial Infarction. Frontiers in Cardiovascular Medicine, vol. 10, 2023, p.918.\n(7) Vieira, S.M., Mendonça, L. F., Farinha, G. J., & Sousa, J. M. C. (2013). Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients. Applied Soft Computing, 13(8), 3494–3504. https://doi.org/10.1016/j.asoc.2013.03.021\n(8) Yang, Zou, H., Wang, M., Zhang, Q., Li, S., & Liang, H. (2023). Mortality prediction among ICU inpatients based on MIMIC-III database results from the conditional medical generative adversarial network. Heliyon, 9(2), e13200–e13200. https://doi.org/10.1016/j.heliyon.2023.e13200\nIntroduction\nSupport Vector Machines (SVM) are a great way to mine data in Electronic Medical Records (EMR). You can use them to find patterns in the data that might be hard to find with regular statistical methods. SVMs can also be used to make models that can use new data to make accurate predictions. It is important to keep in mind, though, that SVMs can be hard to train, especially on big datasets. SVMs can also be responsive to how the SVM kernel and hyperparameters are chosen. Once the SVM model has been trained, it can be used to guess what will happen with new data. For example, a model could be used to figure out how likely it is that a patient will get a certain illness or what will happen to a patient who already has that disease. Before you can use SVMs for data mining in EMRs, you need to prepare the data since the noise and outliers should be taken out of the data. It is also important to feature engineer the data, which will help make new features that may be more useful for the SVM model.\nSVMs can be used to get useful information from the data and to make models that can improve the care of patients. Here are some more reasons why using SVMs for data mining in EMRs is helpful in the clinical setting.  SVMs can determine a model to predict which patients might survive a severe illness in the hospital. This is important for data mining in EMRs, where the data is often complicated since SVMs can handle noise and errors well. This is important for data mining in EMRs because the data may have errors or missing data. Also, models that are easy to understand can be made with SVMs which is important to medical providers so they can explain it to patients and their family.  This is important for data mining in EMRs because it would be helpful to know how the models work, so we can have evidence to support the results. Overall, SVMs are very helpful tools for mining data in EMRs since you get useful information from the data to make models that can improve the care of patients. This can assist in predicting those patients at risk for mortality or death in the Intensive Care Unit (ICU) which are the sickest of the patients."
  },
  {
    "objectID": "Journal/brad.html#summary-of-articles",
    "href": "Journal/brad.html#summary-of-articles",
    "title": "SVM application in Data Mining in EMR",
    "section": "",
    "text": "Support vector machines (SVMs) are a powerful method for machine learning that can be used for data mining. There are several different SVM kernels, and it is not always clear which one is best for a certain job. The goal of this paper is to help data scientists pick the best SVM kernel for a given job. The authors looked at how well different SVM models did at classification, regression, and clustering, among other data mining tasks. They used both real-world data and data that they made up themselves. The article by Xu et al. aimed to see how well various SVM kernels did at data mining jobs. They found that SVM with the RBF kernel did the best job at most data mining tasks. However, they also found that the performance of the different SVM kernels relies on the task and data set. One problem with this study is that there were only a few data mining jobs carried out. (5)\nMy next journal suggested a new SVM algorithm for jobs related to data mining. This is important since SVM is a powerful machine learning method, but they can be hard to train, especially on big datasets. The goal of this study is to suggest a new SVM algorithm that works better for data mining. They came up with a new SV algorithm that is made for data mining jobs. The program uses several methods to improve how well SVM training works. They tested how well their new SVM algorithm did at classification, regression, and grouping, among other tasks in data mining. They found that their new SVM algorithm was better at most data mining jobs than other SVM algorithms. But this algorithm has a weakness in that it is harder to understand than other SVM algorithms. (3)\nIn addition, Zhou et al wrote about deep mining of electronic medical data using support vector machines to predict the prognosis of severe, acute myocardial infarction. The authors talked about how the MIMIC-3 database is used to find the 13 markers for heart attack cases. They compared SVM algorithms and found that the model was about 92% accurate. They use this model to pull out certain features from the EMR and identify which patients will have a MI. They said that this helps doctors figure out the classification regression parts of a disease outlook. (6)\nMy next piece was about how Fouodo et al and others used support vector machines for survival analysis with R. They used the survivalSVM package to do three different kinds of survival analysis. They used both regression and ranking, which is a mix of the two. The next way to find the constraints was to use regression followed by Cox proportional hazard models. They stated that the SVM worked about as well as other methods on the datasets they used. So, this R package makes it quick and easy to find out how likely a patient is to live. (2)\nAnother article was called “Using Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records.” The goal of this work is to show how support vector machines (SVMs) in electronic medical records (EMRs) can be used to classify diabetes mellitus. This study looked at how well SVMs can classify diabetes because they have been good at diagnosing other diseases from electronic medical records (EMRs). The writers used EMRs from both people with and without diabetes to train an SVM model. During preparation, noise and outliers were first taken out of the EMRs. The SVM model was then trained with the help of guided learning. (1)\nThe next journal discussed a way to predict hospital readmissions using support vector machines. The goal of this study is to make a support vector machine (SVM) model that can predict a patient's return to the hospital. The importance was that going back to the hospital is a deadly problem in health care, and it can be expensive for patients. A reliable predictor of hospital readmission could help hospitals find people who are at risk and give them treatment to keep them from going back to the hospital. A solution is that a collection of electronic medical records (EMRs) was used to train an SVM model. First, during preprocessing, abnormalities were taken out of the EMRs. The SVM model was then trained with the help of guided learning and separated the information into two groups. With the SVM model, this included readmitted patients who had to go back to the hospital. (4)\nSVM was used by Vieira et al. to divide data into two groups. The algorithm maps the raw data to a high-dimensional feature space, where a linear classification surface is made. The SVM method then tries to find the best hyperplane that separates the two types of data by the most. The margin is the distance between the hyperplane and the data points in each group that are closest to the hyperplane. The SVM algorithm also uses a kernel function to move the raw data into a space with more dimensions, where it is easier to separate. The kernel function is a piece of math that figures out how similar two data points are to each other. SVM also uses regularization to control the trade-off between making the margin as big as possible and making the classification mistake as small as possible. The SVM algorithm learns from a set of labeled data, where each data point has a label that tells what group it belongs to. Once the SVM algorithm has been taught, it can be used to put new data points that have not been labeled into one of the two groups. (7)\nYang et al. evaluated the performance a version of GAN called conditional medical GAN (C-med GAN) could determine who would die among ICU patients. The study used data from the Medical Information Mart for Intensive Care III (MIMIC-III) database and compared the success of the C-med GAN with some baseline models, such as the simplified acute physiology score II (SAPS II), the support vector machine (SVM), and the multilayer perceptron (MLP). The dataset was split into three sizes, and a 5-fold grid search cross-validation process was used to find the best hyperparameters and then the best model selection for the C-med GAN. Area under the precision-recall curve (PR-AUC), area under the receiver operating characteristic curve (ROC-AUC), and F1 score were used to measure the C-med GAN’s accuracy. The study came up with a helpful method to use SAPS II results to directly estimate how long a patient will live. The results of this study could be used in intensive care to make it easier to predict mortality in the ICU. (8)\nReferences\n(1) Adeoye, Abiodun O., et al. Utilizing Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records. International Journal of Advanced Computer Science and Information Technology (IJACSIT), vol. 11, no. 10, 2021, pp. 102-114.\n(2) Fouodo, Cesaire, et al. Support Vector Machines for Survival Analysis with R. R Journal, vol. 14, no. 2, 2022, pp. 92-107.\n(3) Hu, Xiangfen, Wei Huang, and Qiang Wu. A New Support Vector Machine Algorithm for Data Mining.\" Knowledge-Based Systems, vol. 112, 2016, pp. 118-128.\n(4) Ismail, Gaber A., et al. An Approach Using Support Vector Machines to Predict Hospital Readmission.\" Journal of Medical Systems, vol. 44, no. 9, 2020, pp. 1-10.\n(5) Xu, Fei, Lihong Li, and Zhihua Zhou. SVM Kernels for Data Mining: A Comparative Study.\" Proceedings of the 2010 SIAM International Conference on Data Mining (SDM), 2010, 585-596.\n(6) Zhou, Xingyu, et al. Using Support Vector Machines for Deep Mining of Electronic Medical Records in Order to Predict Prognosis of Severe, Acute Myocardial Infarction. Frontiers in Cardiovascular Medicine, vol. 10, 2023, p.918.\n(7) Vieira, S.M., Mendonça, L. F., Farinha, G. J., & Sousa, J. M. C. (2013). Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients. Applied Soft Computing, 13(8), 3494–3504. https://doi.org/10.1016/j.asoc.2013.03.021\n(8) Yang, Zou, H., Wang, M., Zhang, Q., Li, S., & Liang, H. (2023). Mortality prediction among ICU inpatients based on MIMIC-III database results from the conditional medical generative adversarial network. Heliyon, 9(2), e13200–e13200. https://doi.org/10.1016/j.heliyon.2023.e13200\nIntroduction\nSupport Vector Machines (SVM) are a great way to mine data in Electronic Medical Records (EMR). You can use them to find patterns in the data that might be hard to find with regular statistical methods. SVMs can also be used to make models that can use new data to make accurate predictions. It is important to keep in mind, though, that SVMs can be hard to train, especially on big datasets. SVMs can also be responsive to how the SVM kernel and hyperparameters are chosen. Once the SVM model has been trained, it can be used to guess what will happen with new data. For example, a model could be used to figure out how likely it is that a patient will get a certain illness or what will happen to a patient who already has that disease. Before you can use SVMs for data mining in EMRs, you need to prepare the data since the noise and outliers should be taken out of the data. It is also important to feature engineer the data, which will help make new features that may be more useful for the SVM model.\nSVMs can be used to get useful information from the data and to make models that can improve the care of patients. Here are some more reasons why using SVMs for data mining in EMRs is helpful in the clinical setting.  SVMs can determine a model to predict which patients might survive a severe illness in the hospital. This is important for data mining in EMRs, where the data is often complicated since SVMs can handle noise and errors well. This is important for data mining in EMRs because the data may have errors or missing data. Also, models that are easy to understand can be made with SVMs which is important to medical providers so they can explain it to patients and their family.  This is important for data mining in EMRs because it would be helpful to know how the models work, so we can have evidence to support the results. Overall, SVMs are very helpful tools for mining data in EMRs since you get useful information from the data to make models that can improve the care of patients. This can assist in predicting those patients at risk for mortality or death in the Intensive Care Unit (ICU) which are the sickest of the patients."
  },
  {
    "objectID": "Journal/brad.html#methods",
    "href": "Journal/brad.html#methods",
    "title": "SVM application in Data Mining in EMR",
    "section": "Methods",
    "text": "Methods"
  },
  {
    "objectID": "Journal/brad.html#analysis-and-results",
    "href": "Journal/brad.html#analysis-and-results",
    "title": "SVM application in Data Mining in EMR",
    "section": "Analysis and Results",
    "text": "Analysis and Results\n\nData and Visualisation\nA study was conducted to determine how…\n\n\nCode\n# loading packages \nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(ggthemes)\nlibrary(ggrepel)\nlibrary(dslabs)\n\n\n\n\nCode\n# Load Data\nkable(head(murders))\n\n\n\n\n\nstate\nabb\nregion\npopulation\ntotal\n\n\n\n\nAlabama\nAL\nSouth\n4779736\n135\n\n\nAlaska\nAK\nWest\n710231\n19\n\n\nArizona\nAZ\nWest\n6392017\n232\n\n\nArkansas\nAR\nSouth\n2915918\n93\n\n\nCalifornia\nCA\nWest\n37253956\n1257\n\n\nColorado\nCO\nWest\n5029196\n65\n\n\n\n\n\nCode\nggplot1 = murders %&gt;% ggplot(mapping = aes(x=population/10^6, y=total)) \n\n  ggplot1 + geom_point(aes(col=region), size = 4) +\n  geom_text_repel(aes(label=abb)) +\n  scale_x_log10() +\n  scale_y_log10() +\n  geom_smooth(formula = \"y~x\", method=lm,se = F)+\n  xlab(\"Populations in millions (log10 scale)\") + \n  ylab(\"Total number of murders (log10 scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") +\n  scale_color_discrete(name = \"Region\")+\n      theme_bw()\n\n\n\n\n\n\n\nStatistical Modeling\n\n\nConclusion"
  },
  {
    "objectID": "Journal/brad.html#references",
    "href": "Journal/brad.html#references",
    "title": "SVM application in Data Mining in EMR",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "Journal/brad-08oct2023.html",
    "href": "Journal/brad-08oct2023.html",
    "title": "SVM application in Data Mining in EMR",
    "section": "",
    "text": "Support vector machines (SVMs) are a powerful method for machine learning that can be used for data mining. There are several different SVM kernels, and it is not always clear which one is best for a certain job. The goal of this paper is to help data scientists pick the best SVM kernel for a given job. The authors looked at how well different SVM models did at classification, regression, and clustering, among other data mining tasks. They used both real-world data and data that they made up themselves. The article by Xu et al. aimed to see how well various SVM kernels did at data mining jobs. They found that SVM with the RBF kernel did the best job at most data mining tasks. However, they also found that the performance of the different SVM kernels relies on the task and data set. One problem with this study is that there were only a few data mining jobs carried out. (5)\nMy next journal suggested a new SVM algorithm for jobs related to data mining. This is important since SVM is a powerful machine learning method, but they can be hard to train, especially on big datasets. The goal of this study is to suggest a new SVM algorithm that works better for data mining. They came up with a new SV algorithm that is made for data mining jobs. The program uses several methods to improve how well SVM training works. They tested how well their new SVM algorithm did at classification, regression, and grouping, among other tasks in data mining. They found that their new SVM algorithm was better at most data mining jobs than other SVM algorithms. But this algorithm has a weakness in that it is harder to understand than other SVM algorithms. (3)\nIn addition, Zhou et al wrote about deep mining of electronic medical data using support vector machines to predict the prognosis of severe, acute myocardial infarction. The authors talked about how the MIMIC-3 database is used to find the 13 markers for heart attack cases. They compared SVM algorithms and found that the model was about 92% accurate. They use this model to pull out certain features from the EMR and identify which patients will have a MI. They said that this helps doctors figure out the classification regression parts of a disease outlook. (6)\nMy next piece was about how Fouodo et al and others used support vector machines for survival analysis with R. They used the survivalSVM package to do three different kinds of survival analysis. They used both regression and ranking, which is a mix of the two. The next way to find the constraints was to use regression followed by Cox proportional hazard models. They stated that the SVM worked about as well as other methods on the datasets they used. So, this R package makes it quick and easy to find out how likely a patient is to live. (2)\nAnother article was called “Using Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records.” The goal of this work is to show how support vector machines (SVMs) in electronic medical records (EMRs) can be used to classify diabetes mellitus. This study looked at how well SVMs can classify diabetes because they have been good at diagnosing other diseases from electronic medical records (EMRs). The writers used EMRs from both people with and without diabetes to train an SVM model. During preparation, noise and outliers were first taken out of the EMRs. The SVM model was then trained with the help of guided learning. (1)\nThe next journal discussed a way to predict hospital readmissions using support vector machines. The goal of this study is to make a support vector machine (SVM) model that can predict a patient’s return to the hospital. The importance was that going back to the hospital is a deadly problem in health care, and it can be expensive for patients. A reliable predictor of hospital readmission could help hospitals find people who are at risk and give them treatment to keep them from going back to the hospital. A solution is that a collection of electronic medical records (EMRs) was used to train an SVM model. First, during preprocessing, abnormalities were taken out of the EMRs. The SVM model was then trained with the help of guided learning and separated the information into two groups. With the SVM model, this included readmitted patients who had to go back to the hospital. (4)\nSVM was used by Vieira et al. to divide data into two groups. The algorithm maps the raw data to a high-dimensional feature space, where a linear classification surface is made. The SVM method then tries to find the best hyperplane that separates the two types of data by the most. The margin is the distance between the hyperplane and the data points in each group that are closest to the hyperplane. The SVM algorithm also uses a kernel function to move the raw data into a space with more dimensions, where it is easier to separate. The kernel function is a piece of math that figures out how similar two data points are to each other. SVM also uses regularization to control the trade-off between making the margin as big as possible and making the classification mistake as small as possible. The SVM algorithm learns from a set of labeled data, where each data point has a label that tells what group it belongs to. Once the SVM algorithm has been taught, it can be used to put new data points that have not been labeled into one of the two groups. (7)\nYang et al. evaluated the performance a version of GAN called conditional medical GAN (C-med GAN) could determine who would die among ICU patients. The study used data from the Medical Information Mart for Intensive Care III (MIMIC-III) database and compared the success of the C-med GAN with some baseline models, such as the simplified acute physiology score II (SAPS II), the support vector machine (SVM), and the multilayer perceptron (MLP). The dataset was split into three sizes, and a 5-fold grid search cross-validation process was used to find the best hyperparameters and then the best model selection for the C-med GAN. Area under the precision-recall curve (PR-AUC), area under the receiver operating characteristic curve (ROC-AUC), and F1 score were used to measure the C-med GAN’s accuracy. The study came up with a helpful method to use SAPS II results to directly estimate how long a patient will live. The results of this study could be used in intensive care to make it easier to predict mortality in the ICU. (8)\nThe next article was about SVM and classifying cancer diagnosis using decision support. Chandrashekar et al compared SVM to KNN, RF, bayes and a decision tree to see which algorithm was better at classifying cancer in patients. They pulled from 20 cancer databased and tried to recognize the pattern from genetic variants.  They came up with a preliminary model after much data cleanup and feature selection with 5 out of the 88 possible features.  They improved the model with the use of 16 important features and divided up to two model groups.  The authors trained on 20 datasets with one and just 15 with the other but still have 5 dataset to test from. The F1 score was 0.17 for the SVM model and 0.11, 0.34 for recall.  So, overall SVM has about 88% accuracy, which was better than the other ML algorithms and best performance overall. (9)\nMandakini et al used SVM to predict Heart and Liver disease, but they added Swarm Optimization alongside it.  They took their data from the UCI machine learning repository and then looked at the accuracy, error and recall of the model.  The authors stated that the SVM was a better classifier for predicting liver disorders and cardiac arrythmias.  They first normalized the dataset then trained it and constructed a SVM mode, then added a data structure.  Later, they optimized the SVM model using the Crazy Particle Swarm Optimization (CPSO).  Then they added a Cauchy mutation and constructed their model.  Finally, they could test and classify their data with an F1 score of 89.55 for just SVM on heart disease and 77.24 on liver disease and 90% precision. They were able to increase that by 10-20 points on the F1 score by adding the CPSO on top of the SVM model. (10)\nReferences\n(1) Adeoye, Abiodun O., et al. Utilizing Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records. International Journal of Advanced Computer Science and Information Technology (IJACSIT), vol. 11, no. 10, 2021, pp. 102-114.\n(2) Fouodo, Cesaire, et al. Support Vector Machines for Survival Analysis with R. R Journal, vol. 14, no. 2, 2022, pp. 92-107.\n(3) Hu, Xiangfen, Wei Huang, and Qiang Wu. A New Support Vector Machine Algorithm for Data Mining.” Knowledge-Based Systems, vol. 112, 2016, pp. 118-128.\n(4) Ismail, Gaber A., et al. An Approach Using Support Vector Machines to Predict Hospital Readmission.” Journal of Medical Systems, vol. 44, no. 9, 2020, pp. 1-10.\n(5) Xu, Fei, Lihong Li, and Zhihua Zhou. SVM Kernels for Data Mining: A Comparative Study.” Proceedings of the 2010 SIAM International Conference on Data Mining (SDM), 2010, 585-596.\n(6) Zhou, Xingyu, et al. Using Support Vector Machines for Deep Mining of Electronic Medical Records in Order to Predict Prognosis of Severe, Acute Myocardial Infarction. Frontiers in Cardiovascular Medicine, vol. 10, 2023, p.918.\n(7) Vieira, S.M., Mendonça, L. F., Farinha, G. J., & Sousa, J. M. C. (2013). Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients. Applied Soft Computing, 13(8), 3494–3504. https://doi.org/10.1016/j.asoc.2013.03.021\n(8) Yang, Zou, H., Wang, M., Zhang, Q., Li, S., & Liang, H. (2023). Mortality prediction among ICU inpatients based on MIMIC-III database results from the conditional medical generative adversarial network. Heliyon, 9(2), e13200–e13200. https://doi.org/10.1016/j.heliyon.2023.e13200\n(9) Chandrashekar, K., Setlur, A. S., Sabhapathi C, A., Raiker, S. S., Singh, S., & Niranjan, V. (2023). Decision Support System and Web-Application Using Supervised Machine Learning Algorithms for Easy Cancer Classifications. Cancer informatics, 22, 11769351221147244. https://doi.org/10.1177/11769351221147244\n(10) Mandakini Priyadarshani Behera, Archana Sarangi, Debahuti Mishra, Shubhendu Kumar Sarangi, A Hybrid Machine Learning algorithm for Heart and Liver Disease Prediction Using Modified Particle Swarm Optimization with Support Vector Machine, Procedia Computer Science, Volume 218, 2023,Pages 818-827, https://doi.org/10.1016/j.procs.2023.01.062.\nIntroduction\nSupport Vector Machines (SVM) are an efficient way to mine data by serving as a classification algorithm in Electronic Medical Records (EMR). We find patterns in the data that might be hidden with other statistical methods. SVMs will be used to make models that can use new data to make accurate predictions. It is important to keep in mind, though, that SVMs can be hard to train, especially on big datasets. SVMs can also be responsive to how the SVM kernel and hyperparameters are chosen. Once the SVM model has been trained, it can be used to guess what will happen with new data. For example, a model could be used to figure out how likely it is that a patient will get a certain illness or what will happen to a patient who already has that disease. Before using SVMs for data mining in EMRs, we need to prepare the data since the noise and outliers should be taken out of the data. It is also important to feature engineer the data, which will help make new features that may be more useful for the SVM model.\nSVMs can be used to get useful information from the data and to make models that can improve the care of patients. SVMs can determine a model to predict which patients might survive a severe illness in the hospital. This is important for data mining in EMRs, where the data is often complicated since SVMs can handle noise and errors well. This is important for data mining in EMRs because the data may have errors or missing data. This is important for data mining in EMRs because it would be helpful to know how the models work, so we can have evidence to support the results. Overall, SVMs can mine data in EMRs to create a new SVM that can improve the care of patients by modeling who is at increased risk of death. This can assist in predicting the probability of those patients at risk for mortality in the Intensive Care Unit (ICU)."
  },
  {
    "objectID": "Journal/brad-08oct2023.html#summary-of-articles",
    "href": "Journal/brad-08oct2023.html#summary-of-articles",
    "title": "SVM application in Data Mining in EMR",
    "section": "",
    "text": "Support vector machines (SVMs) are a powerful method for machine learning that can be used for data mining. There are several different SVM kernels, and it is not always clear which one is best for a certain job. The goal of this paper is to help data scientists pick the best SVM kernel for a given job. The authors looked at how well different SVM models did at classification, regression, and clustering, among other data mining tasks. They used both real-world data and data that they made up themselves. The article by Xu et al. aimed to see how well various SVM kernels did at data mining jobs. They found that SVM with the RBF kernel did the best job at most data mining tasks. However, they also found that the performance of the different SVM kernels relies on the task and data set. One problem with this study is that there were only a few data mining jobs carried out. (5)\nMy next journal suggested a new SVM algorithm for jobs related to data mining. This is important since SVM is a powerful machine learning method, but they can be hard to train, especially on big datasets. The goal of this study is to suggest a new SVM algorithm that works better for data mining. They came up with a new SV algorithm that is made for data mining jobs. The program uses several methods to improve how well SVM training works. They tested how well their new SVM algorithm did at classification, regression, and grouping, among other tasks in data mining. They found that their new SVM algorithm was better at most data mining jobs than other SVM algorithms. But this algorithm has a weakness in that it is harder to understand than other SVM algorithms. (3)\nIn addition, Zhou et al wrote about deep mining of electronic medical data using support vector machines to predict the prognosis of severe, acute myocardial infarction. The authors talked about how the MIMIC-3 database is used to find the 13 markers for heart attack cases. They compared SVM algorithms and found that the model was about 92% accurate. They use this model to pull out certain features from the EMR and identify which patients will have a MI. They said that this helps doctors figure out the classification regression parts of a disease outlook. (6)\nMy next piece was about how Fouodo et al and others used support vector machines for survival analysis with R. They used the survivalSVM package to do three different kinds of survival analysis. They used both regression and ranking, which is a mix of the two. The next way to find the constraints was to use regression followed by Cox proportional hazard models. They stated that the SVM worked about as well as other methods on the datasets they used. So, this R package makes it quick and easy to find out how likely a patient is to live. (2)\nAnother article was called “Using Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records.” The goal of this work is to show how support vector machines (SVMs) in electronic medical records (EMRs) can be used to classify diabetes mellitus. This study looked at how well SVMs can classify diabetes because they have been good at diagnosing other diseases from electronic medical records (EMRs). The writers used EMRs from both people with and without diabetes to train an SVM model. During preparation, noise and outliers were first taken out of the EMRs. The SVM model was then trained with the help of guided learning. (1)\nThe next journal discussed a way to predict hospital readmissions using support vector machines. The goal of this study is to make a support vector machine (SVM) model that can predict a patient’s return to the hospital. The importance was that going back to the hospital is a deadly problem in health care, and it can be expensive for patients. A reliable predictor of hospital readmission could help hospitals find people who are at risk and give them treatment to keep them from going back to the hospital. A solution is that a collection of electronic medical records (EMRs) was used to train an SVM model. First, during preprocessing, abnormalities were taken out of the EMRs. The SVM model was then trained with the help of guided learning and separated the information into two groups. With the SVM model, this included readmitted patients who had to go back to the hospital. (4)\nSVM was used by Vieira et al. to divide data into two groups. The algorithm maps the raw data to a high-dimensional feature space, where a linear classification surface is made. The SVM method then tries to find the best hyperplane that separates the two types of data by the most. The margin is the distance between the hyperplane and the data points in each group that are closest to the hyperplane. The SVM algorithm also uses a kernel function to move the raw data into a space with more dimensions, where it is easier to separate. The kernel function is a piece of math that figures out how similar two data points are to each other. SVM also uses regularization to control the trade-off between making the margin as big as possible and making the classification mistake as small as possible. The SVM algorithm learns from a set of labeled data, where each data point has a label that tells what group it belongs to. Once the SVM algorithm has been taught, it can be used to put new data points that have not been labeled into one of the two groups. (7)\nYang et al. evaluated the performance a version of GAN called conditional medical GAN (C-med GAN) could determine who would die among ICU patients. The study used data from the Medical Information Mart for Intensive Care III (MIMIC-III) database and compared the success of the C-med GAN with some baseline models, such as the simplified acute physiology score II (SAPS II), the support vector machine (SVM), and the multilayer perceptron (MLP). The dataset was split into three sizes, and a 5-fold grid search cross-validation process was used to find the best hyperparameters and then the best model selection for the C-med GAN. Area under the precision-recall curve (PR-AUC), area under the receiver operating characteristic curve (ROC-AUC), and F1 score were used to measure the C-med GAN’s accuracy. The study came up with a helpful method to use SAPS II results to directly estimate how long a patient will live. The results of this study could be used in intensive care to make it easier to predict mortality in the ICU. (8)\nThe next article was about SVM and classifying cancer diagnosis using decision support. Chandrashekar et al compared SVM to KNN, RF, bayes and a decision tree to see which algorithm was better at classifying cancer in patients. They pulled from 20 cancer databased and tried to recognize the pattern from genetic variants.  They came up with a preliminary model after much data cleanup and feature selection with 5 out of the 88 possible features.  They improved the model with the use of 16 important features and divided up to two model groups.  The authors trained on 20 datasets with one and just 15 with the other but still have 5 dataset to test from. The F1 score was 0.17 for the SVM model and 0.11, 0.34 for recall.  So, overall SVM has about 88% accuracy, which was better than the other ML algorithms and best performance overall. (9)\nMandakini et al used SVM to predict Heart and Liver disease, but they added Swarm Optimization alongside it.  They took their data from the UCI machine learning repository and then looked at the accuracy, error and recall of the model.  The authors stated that the SVM was a better classifier for predicting liver disorders and cardiac arrythmias.  They first normalized the dataset then trained it and constructed a SVM mode, then added a data structure.  Later, they optimized the SVM model using the Crazy Particle Swarm Optimization (CPSO).  Then they added a Cauchy mutation and constructed their model.  Finally, they could test and classify their data with an F1 score of 89.55 for just SVM on heart disease and 77.24 on liver disease and 90% precision. They were able to increase that by 10-20 points on the F1 score by adding the CPSO on top of the SVM model. (10)\nReferences\n(1) Adeoye, Abiodun O., et al. Utilizing Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records. International Journal of Advanced Computer Science and Information Technology (IJACSIT), vol. 11, no. 10, 2021, pp. 102-114.\n(2) Fouodo, Cesaire, et al. Support Vector Machines for Survival Analysis with R. R Journal, vol. 14, no. 2, 2022, pp. 92-107.\n(3) Hu, Xiangfen, Wei Huang, and Qiang Wu. A New Support Vector Machine Algorithm for Data Mining.” Knowledge-Based Systems, vol. 112, 2016, pp. 118-128.\n(4) Ismail, Gaber A., et al. An Approach Using Support Vector Machines to Predict Hospital Readmission.” Journal of Medical Systems, vol. 44, no. 9, 2020, pp. 1-10.\n(5) Xu, Fei, Lihong Li, and Zhihua Zhou. SVM Kernels for Data Mining: A Comparative Study.” Proceedings of the 2010 SIAM International Conference on Data Mining (SDM), 2010, 585-596.\n(6) Zhou, Xingyu, et al. Using Support Vector Machines for Deep Mining of Electronic Medical Records in Order to Predict Prognosis of Severe, Acute Myocardial Infarction. Frontiers in Cardiovascular Medicine, vol. 10, 2023, p.918.\n(7) Vieira, S.M., Mendonça, L. F., Farinha, G. J., & Sousa, J. M. C. (2013). Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients. Applied Soft Computing, 13(8), 3494–3504. https://doi.org/10.1016/j.asoc.2013.03.021\n(8) Yang, Zou, H., Wang, M., Zhang, Q., Li, S., & Liang, H. (2023). Mortality prediction among ICU inpatients based on MIMIC-III database results from the conditional medical generative adversarial network. Heliyon, 9(2), e13200–e13200. https://doi.org/10.1016/j.heliyon.2023.e13200\n(9) Chandrashekar, K., Setlur, A. S., Sabhapathi C, A., Raiker, S. S., Singh, S., & Niranjan, V. (2023). Decision Support System and Web-Application Using Supervised Machine Learning Algorithms for Easy Cancer Classifications. Cancer informatics, 22, 11769351221147244. https://doi.org/10.1177/11769351221147244\n(10) Mandakini Priyadarshani Behera, Archana Sarangi, Debahuti Mishra, Shubhendu Kumar Sarangi, A Hybrid Machine Learning algorithm for Heart and Liver Disease Prediction Using Modified Particle Swarm Optimization with Support Vector Machine, Procedia Computer Science, Volume 218, 2023,Pages 818-827, https://doi.org/10.1016/j.procs.2023.01.062.\nIntroduction\nSupport Vector Machines (SVM) are an efficient way to mine data by serving as a classification algorithm in Electronic Medical Records (EMR). We find patterns in the data that might be hidden with other statistical methods. SVMs will be used to make models that can use new data to make accurate predictions. It is important to keep in mind, though, that SVMs can be hard to train, especially on big datasets. SVMs can also be responsive to how the SVM kernel and hyperparameters are chosen. Once the SVM model has been trained, it can be used to guess what will happen with new data. For example, a model could be used to figure out how likely it is that a patient will get a certain illness or what will happen to a patient who already has that disease. Before using SVMs for data mining in EMRs, we need to prepare the data since the noise and outliers should be taken out of the data. It is also important to feature engineer the data, which will help make new features that may be more useful for the SVM model.\nSVMs can be used to get useful information from the data and to make models that can improve the care of patients. SVMs can determine a model to predict which patients might survive a severe illness in the hospital. This is important for data mining in EMRs, where the data is often complicated since SVMs can handle noise and errors well. This is important for data mining in EMRs because the data may have errors or missing data. This is important for data mining in EMRs because it would be helpful to know how the models work, so we can have evidence to support the results. Overall, SVMs can mine data in EMRs to create a new SVM that can improve the care of patients by modeling who is at increased risk of death. This can assist in predicting the probability of those patients at risk for mortality in the Intensive Care Unit (ICU)."
  },
  {
    "objectID": "Journal/brad-08oct2023.html#methods",
    "href": "Journal/brad-08oct2023.html#methods",
    "title": "SVM application in Data Mining in EMR",
    "section": "Methods",
    "text": "Methods\nMIMIC-III is a , free database with information about the health of more than 40,000 people who stayed in the critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. The information has been stripped of all personal information. The database has information like demographics, measurements of vital signs taken at the bedside (about one data point per hour), lab test results, procedures, medicines, caregiver notes, imaging reports, and deaths (including those that happened after the patient was released from the hospital).\nMIMIC helps with a wide range of analytical studies, including statistics, improving clinical decision rules, and making electronic tools. It is important for three reasons: researchers from anywhere in the world can use it for free, it includes a very large and diverse group of ICU patients, and it has very detailed information like vital signs, lab results, and medications.\nUnder a data use agreement, MIMIC-III combines de-identified, complete clinical data from patients who were admitted to the Beth Israel Deaconess Medical Center in Boston, Massachusetts. This data is then made available to researchers all over the world. Because the data are public, clinical studies can be repeated and made better in ways that would not be possible otherwise.\nThe MIMIC-III database was filled with information that was collected during normal hospital care, so it didn’t add any extra work for nurses or get in the way of their work. Data was taken from a number of places, including:\nCritical care information tools and hospital electronic health record databases have records from the past.\nDeath by the Social Security Administration The main file.\nDuring the time when the data was collected, there were two different critical care information systems: the Philips CareVue Clinical Information System (types M2331A and M1215A; Philips Health-care, Andover, MA) and the iMDsoft MetaVision ICU (iMDsoft, Needham, MA).\nThese tools were where clinical information like:\nPhysiological measures taken by a nurse and time-stamped (for example, recording the heart rate, arterial blood pressure, or breathing rate every hour); care providers’ written notes on the patient’s progress; and medications and fluid balances given through an intravenous drip.\nWith the exception of data about fluid intake, which was very different between the CareVue and MetaVision systems in how it was set up, data was combined when the database tables were made. For data that couldn’t be combined, a suffix is added to show where the data came from. For example, inputs for patients being monitored with the CareVue system are stored in INPUTEVENTS_CV, while inputs for patients being monitored with the Metavision system are stored in INPUTEVENTS_MV. From hospital and lab health record systems, more information was gathered, including:\npatient characteristics and deaths in the hospital.\nSome examples of lab test results are hematology, chemistry, and microbiology.\ndischarge summaries and reports of imaging tests and electrocardiograms.\nInformation about bills, such as International Classification of Diseases, 9th Edition (ICD-9) codes, Diagnosis Related Group (DRG) codes, and Current Procedural Terminology (CPT) codes.\nThe Social Security Administration’s Death Master File was used to find out when people died outside of hospitals.\nBefore the data was put into the MIMIC-III database, it was deidentified using structured data cleaning and date shifting, as required by the Health Insurance Portability and Accountability Act (HIPAA). For structured data to be deidentified, all 18 of the identifying data elements listed in HIPAA had to be taken out. This included areas like the patient’s name, phone number, address, and dates. In particular, times were moved into the future by a random amount for each patient in the same way so that intervals could be kept. This made stays happen between the years 2100 and 2200. Date moving did not change the time of day, the day of the week, or the season. To hide their real ages and follow HIPAA rules, the dates of birth of patients older than 89 were changed. In the database, these patients have ages of over 300 years.\nProtected health information was taken out of free-text fields like diagnostic reports and doctor’s notes using a thoroughly tested deidentification system based on extensive dictionary look-ups and pattern-matching with regular expressions. As more information is collected, this deidentification device keeps getting more parts.\nInstitutional Review Boards at the Beth Israel Deaconess Medical Center in Boston, Massachusetts, and the Massachusetts Institute of Technology in Cambridge, Massachusetts, gave their approval to the project. Patients didn’t have to give their consent because the project didn’t affect professional care and all protected health information was made anonymous.\nMIMIC-III is a set of 26 tables that make up a relational database. Identifiers, which usually end in ‘ID’, are used to link tables together. For example, SUBJECT_ID is used to identify a unique patient, HADM_ID is used to identify a unique hospital admission, and ICUSTAY_ID is used to identify a unique hospital entry to an intensive care unit.\nIn a set of “events” tables, things like notes, lab tests, and fluid balance are kept track of. For example, the OUTPUTEVENTS table has all the information about a patient’s output, while the LABEVENTS table has the results of a patient’s lab tests.\nDictionary tables are those that start with “D_” and give meanings for identifiers. For example, each row of CHARTEVENTS has a single ITEMID that stands for the idea being measured, but it doesn’t have the name of the measurement. By joining CHARTEVENTS and D_ITEMS on ITEMID, you can find out what idea an ITEMID stands for.\nWhen making the MIMIC data model, it was important to find a balance between how easy it was to understand and how close it was to the real world. So, the model is a reflection of the data sources it is based on. The MIMIC database has been changed over time based on user comments. When changes were done, care was taken not to make assumptions about the underlying data. This means that MIMIC-III is a good representation of the raw hospital data.\nFive tables, called ADMISSIONS, PATIENTS, ICUSTAYS, SERVICES, and TRANSFERS, are used to describe and keep track of patient stays. Five more tables, D_CPT, D_ICD_DIAGNOSES, D_ICD_PROCEDURES, D_ITEMS, and D_LABITEMS, are dictionaries that let you look up codes by their meanings. The rest of the tables have information about patient care, like measurements of the patient’s body, notes from caregivers, and payment information.\nMIMIC-III is given as a set of comma-separated value (CSV) files and tools to help import the data into database systems like PostreSQL, MySQL, and MonetDB. Since the database has a lot of knowledge about how to care for patients clinically, it needs to be treated with care and respect.\nhttps://mimic.mit.edu/docs/iv/modules/icu/ is where we got the ICU dataset,\n Patient demographics:\no Age: Median 65.8 years (Q1–Q3: 52.8–77.8 years)\no Sex: 55.9% male\no In-hospital mortality: 11.5%\n Vital signs:\no Heart rate: Median 98 beats per minute (Q1–Q3: 76–120 beats per\nminute)\no Blood pressure: Median systolic blood pressure 134 mmHg (Q1–Q3:\n116–154 mmHg); median diastolic blood pressure 78 mmHg (Q1–Q3:\n66–90 mmHg)\no Respiratory rate: Median 20 breaths per minute (Q1–Q3: 16–24 breaths\nper minute)\no Temperature: Median 36.8 °C (Q1–Q3: 36.5–37.1 °C)\no Oxygen saturation: Median 96% (Q1–Q3: 93–99%)\n Laboratory values:\no White blood cell count: Median 10.5 × 10^9 cells/L (Q1–Q3: 7.5–14.5 ×\n10^9 cells/L)\no Neutrophil count: Median 7.5 × 10^9 cells/L (Q1–Q3: 5.4–11.2 × 10^9\ncells/L)\no Lymphocyte count: Median 1.7 × 10^9 cells/L (Q1–Q3: 1.0–2.5 × 10^9\ncells/L)\no Platelet count: Median 178 × 10^9 cells/L (Q1–Q3: 125–240 × 10^9\ncells/L)\no Creatinine: Median 1.0 mg/dL (Q1–Q3: 0.8–1.3 mg/dL)\no Bilirubin: Median 0.8 mg/dL (Q1–Q3: 0.5–1.2 mg/dL)\no Lactate dehydrogenase: Median 250 U/L (Q1–Q3: 190–330 U/L)"
  },
  {
    "objectID": "Journal/brad-08oct2023.html#analysis-and-results",
    "href": "Journal/brad-08oct2023.html#analysis-and-results",
    "title": "SVM application in Data Mining in EMR",
    "section": "Analysis and Results",
    "text": "Analysis and Results\n\nData and Visualization\n\n\n\n\nStatistical Modeling\n\n\nConclusion"
  },
  {
    "objectID": "Journal/brad-08oct2023.html#references",
    "href": "Journal/brad-08oct2023.html#references",
    "title": "SVM application in Data Mining in EMR",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "Journal/eric.html",
    "href": "Journal/eric.html",
    "title": "Eric’s Journal Entries",
    "section": "",
    "text": "https://www.jstatsoft.org/article/view/v015i09\nThe above article from the Journal of statistical Software outlines what Support Vector Machines are and what their use cases can be. The article continues by leveraging the mathematical equations for classification and regression as well as deployment strategies for data sets in R. The article concludes with examples of code and outputs showcasing the results on the iris data set.\n\n\n\nhttps://seis.bristol.ac.uk/~enicgc/pubs/1999/ijcai_ss.pdf\nThe above speaks to controlling the sensitivity of Support Vector Machines to reduce the number of False Positives/Negatives in the output. Veropoulos, Campbell, and Cristianini go on to detail the difference between Sensitivity and Specificity through various mathematical approaches and analyses. Their research with medical data sets where Box constraints were not statistically significant between the 4 data sets. However, the strain on the algorithm was insignificant and could aid in over all determinations."
  },
  {
    "objectID": "Journal/eric.html#support-vector-machines-in-r-karatzoglou2006",
    "href": "Journal/eric.html#support-vector-machines-in-r-karatzoglou2006",
    "title": "Eric’s Journal Entries",
    "section": "",
    "text": "https://www.jstatsoft.org/article/view/v015i09\nThe above article from the Journal of statistical Software outlines what Support Vector Machines are and what their use cases can be. The article continues by leveraging the mathematical equations for classification and regression as well as deployment strategies for data sets in R. The article concludes with examples of code and outputs showcasing the results on the iris data set."
  },
  {
    "objectID": "Journal/eric.html#controlling-the-sensitivity-of-support-vector-machines-cristianini2002",
    "href": "Journal/eric.html#controlling-the-sensitivity-of-support-vector-machines-cristianini2002",
    "title": "Eric’s Journal Entries",
    "section": "",
    "text": "https://seis.bristol.ac.uk/~enicgc/pubs/1999/ijcai_ss.pdf\nThe above speaks to controlling the sensitivity of Support Vector Machines to reduce the number of False Positives/Negatives in the output. Veropoulos, Campbell, and Cristianini go on to detail the difference between Sensitivity and Specificity through various mathematical approaches and analyses. Their research with medical data sets where Box constraints were not statistically significant between the 4 data sets. However, the strain on the algorithm was insignificant and could aid in over all determinations."
  },
  {
    "objectID": "Journal/eric.html#time-series-prediction-using-support-vector-machines-a-survey-sapankevych2009",
    "href": "Journal/eric.html#time-series-prediction-using-support-vector-machines-a-survey-sapankevych2009",
    "title": "Eric’s Journal Entries",
    "section": "Time Series Prediction Using Support Vector Machines: A Survey (Sapankevych and Sankar 2009)",
    "text": "Time Series Prediction Using Support Vector Machines: A Survey (Sapankevych and Sankar 2009)\nhttps://ieeexplore.ieee.org/abstract/document/4840324\nThis article details the ways in which Support Vector Machines have been utilized to perform time series analysis. I found this very interesting as in my current line of work we are looking for ways to implement more Machine Learning into our models and we do a fair amount of Time Series Analysis. This is a important topic for us as we start to work through the best way we can utilize and implement our model and how to best identify the uses for the model. Additionally this article delves into SVR and how that methodology can also be utilized in Time Series modeling."
  },
  {
    "objectID": "Journal/eric.html#a-comparative-analysis-of-k-nearest-neighbor-genetic-support-vector-machine-decision-tree-and-long-short-term-memory-algorithms-in-machine-learning-bansal2022",
    "href": "Journal/eric.html#a-comparative-analysis-of-k-nearest-neighbor-genetic-support-vector-machine-decision-tree-and-long-short-term-memory-algorithms-in-machine-learning-bansal2022",
    "title": "Eric’s Journal Entries",
    "section": "A comparative analysis of K-Nearest Neighbor, Genetic, Support Vector Machine, Decision Tree, and Long Short Term Memory algorithms in machine learning (Bansal, Goyal, and Choudhary 2022)",
    "text": "A comparative analysis of K-Nearest Neighbor, Genetic, Support Vector Machine, Decision Tree, and Long Short Term Memory algorithms in machine learning (Bansal, Goyal, and Choudhary 2022)\nhttps://www.sciencedirect.com/science/article/pii/S2772662222000261\nThe above article delves into the comparative difference between a few different methodologies relative to Machine Learning, such as KNN, SVM, and LSTM. The article further details the strengths and weaknesses of each and their most practical use cases based on what the implementer is ultimately seeking to achieve. Ultimately this provides more context on how to best utilize our model and to most effectively choose our groups data set so that when we start building our project we are not starting off on the wrong foot."
  },
  {
    "objectID": "Journal/eric.html#fast-training-support-vector-machines-using-parallel-sequential-minimal-optimization-zeng2008",
    "href": "Journal/eric.html#fast-training-support-vector-machines-using-parallel-sequential-minimal-optimization-zeng2008",
    "title": "Eric’s Journal Entries",
    "section": "Fast training Support Vector Machines using parallel sequential minimal optimization (Zeng et al. 2008)",
    "text": "Fast training Support Vector Machines using parallel sequential minimal optimization (Zeng et al. 2008)\nhttps://ieeexplore.ieee.org/abstract/document/4731075\nThe article above details out the various ways we can quickly train SVM models utilizing Sequential Minimal Optimization (SMO) algorithms to reduce the problems that can arise from large scale programming. A parallel SMO method was the primary focus of this paper as it covered the basic functions and algorithms behind the inner workings as applied to an SVM."
  },
  {
    "objectID": "Journal/eric.html#support-vector-machine-accuracy-improvement-with-classification-mohan2020",
    "href": "Journal/eric.html#support-vector-machine-accuracy-improvement-with-classification-mohan2020",
    "title": "Eric’s Journal Entries",
    "section": "Support Vector Machine Accuracy Improvement with Classification (Mohan et al. 2020)",
    "text": "Support Vector Machine Accuracy Improvement with Classification (Mohan et al. 2020)\nhttps://ieeexplore.ieee.org/abstract/document/9242572\nThis article walks through how to successfully setup and run an SVM based on a binary classification problem. Furthermore, it details the inner workings of various types of kernels that can be used to accurately map your planes based on the complexity of the data. I found this particularly interesting as I was wondering about what kernel to build upon and it seems as if an RBF or a Gaussian kernel might be our ticket."
  },
  {
    "objectID": "Journal/eric.html#effectiveness-of-random-search-in-svm-hyper-parameter-tuning-mantovani2015",
    "href": "Journal/eric.html#effectiveness-of-random-search-in-svm-hyper-parameter-tuning-mantovani2015",
    "title": "Eric’s Journal Entries",
    "section": "Effectiveness of Random Search in SVM hyper-parameter tuning (Mantovani et al. 2015)",
    "text": "Effectiveness of Random Search in SVM hyper-parameter tuning (Mantovani et al. 2015)\nhttps://ieeexplore.ieee.org/abstract/document/7280664\nThe authors of this article delve into the theory and techniques of tuning the Hyper-Parameters utilized by the SVM in relation classification, specifically Random Search. Often times this tuning requires a significant time investment with trial and error. It can also require specific knowledge of the data and outcomes associated with the training of the model, and frequently different variables will yield greater results dependent on the outcome. They concluded that given low dimensionality data sets Random Search can yield similar predictive results as more complex tuning methods such as grid search and meta-heuris tics."
  },
  {
    "objectID": "Journal/eric.html#empirical-evaluation-of-resampling-procedures-for-optimising-svm-hyperparameters",
    "href": "Journal/eric.html#empirical-evaluation-of-resampling-procedures-for-optimising-svm-hyperparameters",
    "title": "Eric’s Journal Entries",
    "section": "Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters",
    "text": "Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters\nhttps://www.jmlr.org/papers/volume18/16-174/16-174.pdf\nThe paper delves into the Radial Basis Function (RBF) Kernel of SVM’s and discusses the need for Hyper-Parameter selection between a regularization and a governing parameter. The paper goes on to detail that there is no generally accepted means for the optimization of Hyper-Parameters. Once approach is to continuously re-sample the initial training data from the data set utilizing different methods and then evaluate them. Ultimately the paper concludes that based on the research and testing of 17 varying methods of 121 data sets there is no definite solution to determine a one size fits all approach to selection of parameters based on an expected outcome."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html",
    "href": "MIMIC_ICU_Data/dataMaid_data.html",
    "title": "data",
    "section": "",
    "text": "The dataset examined has the following dimensions:\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nNumber of observations\n4559\n\n\nNumber of variables\n106\n\n\n\n\n\nThe following variable checks were performed, depending on the data type of each variable:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \ncharacter\nfactor\nlabelled\nhaven labelled\nnumeric\ninteger\nlogical\nDate\n\n\n\n\nIdentify miscoded missing values\n×\n×\n×\n×\n×\n×\n\n×\n\n\nIdentify prefixed and suffixed whitespace\n×\n×\n×\n×\n\n\n\n\n\n\nIdentify levels with &lt; 6 obs.\n×\n×\n×\n×\n\n\n\n\n\n\nIdentify case issues\n×\n×\n×\n×\n\n\n\n\n\n\nIdentify misclassified numeric or integer variables\n×\n×\n×\n×\n\n\n\n\n\n\nIdentify outliers\n\n\n\n\n×\n×\n\n×\n\n\n\nPlease note that all numerical values in the following have been rounded to 2 decimals."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#icustay_id1",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#icustay_id1",
    "title": "data",
    "section": "icustay_id…1",
    "text": "icustay_id…1\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4559\n\n\nMedian\n251008\n\n\n1st and 3rd quartiles\n225575.5; 275526\n\n\nMin. and max.\n200075; 299998"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#hadm_id2",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#hadm_id2",
    "title": "data",
    "section": "hadm_id…2",
    "text": "hadm_id…2\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4559\n\n\nMedian\n149643\n\n\n1st and 3rd quartiles\n125389; 175033\n\n\nMin. and max.\n100003; 199962"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#intime",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#intime",
    "title": "data",
    "section": "intime",
    "text": "intime\n\nThe variable is a key (distinct values for each observation)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#outtime",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#outtime",
    "title": "data",
    "section": "outtime",
    "text": "outtime\n\nThe variable is a key (distinct values for each observation)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#dbsource",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#dbsource",
    "title": "data",
    "section": "dbsource",
    "text": "dbsource\n\nThe variable only takes one (non-missing) value: \"metavision\". The variable contains 0 % missing observations."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#suspected_infection_time_poe",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#suspected_infection_time_poe",
    "title": "data",
    "section": "suspected_infection_time_poe",
    "text": "suspected_infection_time_poe\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\ncharacter\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4558\n\n\nMode\n“18/8/2159 00:00:00”\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following levels have at most five observations: \"1/1/2118 1:02\", \"1/1/2118 14:36\", \"1/1/2125 14:05\", \"1/1/2132 21:00\", \"1/1/2135 15:55\", …, \"9/9/2184 10:25\", \"9/9/2185 18:24\", \"9/9/2186 18:18\", \"9/9/2188 1:05\", \"9/9/2198 18:30\" (4548 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#suspected_infection_time_poe_days",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#suspected_infection_time_poe_days",
    "title": "data",
    "section": "suspected_infection_time_poe_days",
    "text": "suspected_infection_time_poe_days\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4370\n\n\nMedian\n0.03\n\n\n1st and 3rd quartiles\n-0.08; 0.16\n\n\nMin. and max.\n-0.99; 1\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"-0.99\", \"-0.98\", \"-0.93\", \"-0.93\", \"-0.92\", …, \"0.99\", \"0.99\", \"0.99\", \"0.99\", \"1\" (519 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#specimen_poe",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#specimen_poe",
    "title": "data",
    "section": "specimen_poe",
    "text": "specimen_poe\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\ncharacter\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n35\n\n\nMode\n“BLOOD CULTURE”\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following levels have at most five observations: \"ABSCESS\", \"BILE\", \"Blood (EBV)\", \"Blood (Malaria)\", \"Blood (Toxo)\", …, \"NEOPLASTIC BLOOD\", \"PLEURAL FLUID\", \"RAPID RESPIRATORY VIRAL ANTIGEN TEST\", \"Rapid Respiratory Viral Screen & Culture\", \"URINE,KIDNEY\" (10 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#positiveculture_poe",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#positiveculture_poe",
    "title": "data",
    "section": "positiveculture_poe",
    "text": "positiveculture_poe\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#antibiotic_time_poe",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#antibiotic_time_poe",
    "title": "data",
    "section": "antibiotic_time_poe",
    "text": "antibiotic_time_poe\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\ncharacter\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4274\n\n\nMode\n“1/4/2191 0:00”\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following levels have at most five observations: \"1/1/2111 0:00\", \"1/1/2116 0:00\", \"1/1/2130 0:00\", \"1/1/2134 0:00\", \"1/1/2135 0:00\", …, \"9/9/2181 0:00\", \"9/9/2185 0:00\", \"9/9/2187 0:00\", \"9/9/2189 0:00\", \"9/9/2198 0:00\" (4264 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#blood_culture_time",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#blood_culture_time",
    "title": "data",
    "section": "blood_culture_time",
    "text": "blood_culture_time\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\ncharacter\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4558\n\n\nMode\n“18/8/2159 00:00:00”\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following levels have at most five observations: \"1/1/2113 20:45\", \"1/1/2118 1:02\", \"1/1/2118 14:36\", \"1/1/2125 14:05\", \"1/1/2132 21:00\", …, \"9/9/2184 10:25\", \"9/9/2185 18:24\", \"9/9/2186 18:18\", \"9/9/2188 1:05\", \"9/9/2198 18:30\" (4548 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#blood_culture_positive",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#blood_culture_positive",
    "title": "data",
    "section": "blood_culture_positive",
    "text": "blood_culture_positive\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#age",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#age",
    "title": "data",
    "section": "age",
    "text": "age\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n3831\n\n\nMedian\n66.59\n\n\n1st and 3rd quartiles\n53.76; 79.53\n\n\nMin. and max.\n16.78; 91.4"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#gender",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#gender",
    "title": "data",
    "section": "gender",
    "text": "gender\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\ncharacter\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“M”"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#is_male",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#is_male",
    "title": "data",
    "section": "is_male",
    "text": "is_male\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“1”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#ethnicity",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#ethnicity",
    "title": "data",
    "section": "ethnicity",
    "text": "ethnicity\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\ncharacter\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n41\n\n\nMode\n“WHITE”\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following levels have at most five observations: \"AMERICAN INDIAN/ALASKA NATIVE\", \"AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE\", \"ASIAN - CAMBODIAN\", \"ASIAN - FILIPINO\", \"ASIAN - JAPANESE\", …, \"HISPANIC/LATINO - SALVADORAN\", \"NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER\", \"SOUTH AMERICAN\", \"WHITE - BRAZILIAN\", \"WHITE - EASTERN EUROPEAN\" (11 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#race_white",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#race_white",
    "title": "data",
    "section": "race_white",
    "text": "race_white\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“1”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#race_black",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#race_black",
    "title": "data",
    "section": "race_black",
    "text": "race_black\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#race_hispanic",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#race_hispanic",
    "title": "data",
    "section": "race_hispanic",
    "text": "race_hispanic\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#race_other",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#race_other",
    "title": "data",
    "section": "race_other",
    "text": "race_other\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#metastatic_cancer",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#metastatic_cancer",
    "title": "data",
    "section": "metastatic_cancer",
    "text": "metastatic_cancer\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#diabetes",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#diabetes",
    "title": "data",
    "section": "diabetes",
    "text": "diabetes\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#first_service",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#first_service",
    "title": "data",
    "section": "first_service",
    "text": "first_service\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\ncharacter\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n13\n\n\nMode\n“MED”"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#hospital_expire_flag",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#hospital_expire_flag",
    "title": "data",
    "section": "hospital_expire_flag",
    "text": "hospital_expire_flag\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#thirtyday_expire_flag",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#thirtyday_expire_flag",
    "title": "data",
    "section": "thirtyday_expire_flag",
    "text": "thirtyday_expire_flag\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#icu_los",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#icu_los",
    "title": "data",
    "section": "icu_los",
    "text": "icu_los\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4401\n\n\nMedian\n2.82\n\n\n1st and 3rd quartiles\n1.61; 5.87\n\n\nMin. and max.\n0.17; 101.74\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.17\", \"0.25\", \"0.25\", \"0.3\", \"0.31\", …, \"59.47\", \"60.94\", \"61.93\", \"79.11\", \"101.74\" (96 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#hosp_los",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#hosp_los",
    "title": "data",
    "section": "hosp_los",
    "text": "hosp_los\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4191\n\n\nMedian\n7.79\n\n\n1st and 3rd quartiles\n4.73; 13.03\n\n\nMin. and max.\n-0.43; 206.43\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"-0.43\", \"-0.06\", \"0.11\", \"0.17\", \"0.18\", …, \"100.49\", \"101.62\", \"123.13\", \"133.1\", \"206.43\" (207 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_angus",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_angus",
    "title": "data",
    "section": "sepsis_angus",
    "text": "sepsis_angus\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“1”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_martin",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_martin",
    "title": "data",
    "section": "sepsis_martin",
    "text": "sepsis_martin\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_explicit",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_explicit",
    "title": "data",
    "section": "sepsis_explicit",
    "text": "sepsis_explicit\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#septic_shock_explicit",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#septic_shock_explicit",
    "title": "data",
    "section": "septic_shock_explicit",
    "text": "septic_shock_explicit\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#severe_sepsis_explicit",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#severe_sepsis_explicit",
    "title": "data",
    "section": "severe_sepsis_explicit",
    "text": "severe_sepsis_explicit\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_nqf",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_nqf",
    "title": "data",
    "section": "sepsis_nqf",
    "text": "sepsis_nqf\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_cdc",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_cdc",
    "title": "data",
    "section": "sepsis_cdc",
    "text": "sepsis_cdc\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“1”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_cdc_simple",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sepsis_cdc_simple",
    "title": "data",
    "section": "sepsis_cdc_simple",
    "text": "sepsis_cdc_simple\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“1”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#elixhauser_hospital",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#elixhauser_hospital",
    "title": "data",
    "section": "elixhauser_hospital",
    "text": "elixhauser_hospital\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n49\n\n\nMedian\n4\n\n\n1st and 3rd quartiles\n0; 9\n\n\nMin. and max.\n-23; 30\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"-23\", \"-21\", \"-20\", \"-19\", \"-16\", …, \"24\", \"25\", \"27\", \"28\", \"30\" (3 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#vent",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#vent",
    "title": "data",
    "section": "vent",
    "text": "vent\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“1”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sofa",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sofa",
    "title": "data",
    "section": "sofa",
    "text": "sofa\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n20\n\n\nMedian\n5\n\n\n1st and 3rd quartiles\n3; 7\n\n\nMin. and max.\n2; 21\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"18\", \"19\", \"20\", \"21\"."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#lods",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#lods",
    "title": "data",
    "section": "lods",
    "text": "lods\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n21\n\n\nMedian\n5\n\n\n1st and 3rd quartiles\n3; 7\n\n\nMin. and max.\n0; 20\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sirs",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sirs",
    "title": "data",
    "section": "sirs",
    "text": "sirs\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n5\n\n\nMode\n“3”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#qsofa",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#qsofa",
    "title": "data",
    "section": "qsofa",
    "text": "qsofa\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4\n\n\nMode\n“2”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#qsofa_sysbp_score",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#qsofa_sysbp_score",
    "title": "data",
    "section": "qsofa_sysbp_score",
    "text": "qsofa_sysbp_score\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n7 (0.15 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“1”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#qsofa_gcs_score",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#qsofa_gcs_score",
    "title": "data",
    "section": "qsofa_gcs_score",
    "text": "qsofa_gcs_score\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n3 (0.07 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#qsofa_resprate_score",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#qsofa_resprate_score",
    "title": "data",
    "section": "qsofa_resprate_score",
    "text": "qsofa_resprate_score\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1 (0.02 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“1”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#aniongap_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#aniongap_min",
    "title": "data",
    "section": "aniongap_min",
    "text": "aniongap_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n14 (0.31 %)\n\n\nNumber of unique values\n34\n\n\nMedian\n12\n\n\n1st and 3rd quartiles\n10; 15\n\n\nMin. and max.\n3; 42\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"3\", \"4\", \"5\", \"6\", \"29\", …, \"32\", \"33\", \"35\", \"36\", \"42\" (2 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#aniongap_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#aniongap_max",
    "title": "data",
    "section": "aniongap_max",
    "text": "aniongap_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n14 (0.31 %)\n\n\nNumber of unique values\n43\n\n\nMedian\n16\n\n\n1st and 3rd quartiles\n13; 19\n\n\nMin. and max.\n6; 56\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"6\", \"7\", \"32\", \"33\", \"34\", …, \"44\", \"46\", \"47\", \"54\", \"56\" (9 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#bicarbonate_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#bicarbonate_min",
    "title": "data",
    "section": "bicarbonate_min",
    "text": "bicarbonate_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n3 (0.07 %)\n\n\nNumber of unique values\n39\n\n\nMedian\n21\n\n\n1st and 3rd quartiles\n18; 24\n\n\nMin. and max.\n5; 45\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"5\", \"6\", \"7\", \"8\", \"34\", …, \"38\", \"39\", \"40\", \"41\", \"45\" (3 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#bicarbonate_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#bicarbonate_max",
    "title": "data",
    "section": "bicarbonate_max",
    "text": "bicarbonate_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n3 (0.07 %)\n\n\nNumber of unique values\n42\n\n\nMedian\n24\n\n\n1st and 3rd quartiles\n22; 27\n\n\nMin. and max.\n8; 49\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"8\", \"9\", \"10\", \"11\", \"12\", …, \"44\", \"45\", \"46\", \"47\", \"49\" (11 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#creatinine_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#creatinine_min",
    "title": "data",
    "section": "creatinine_min",
    "text": "creatinine_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n2 (0.04 %)\n\n\nNumber of unique values\n97\n\n\nMedian\n1\n\n\n1st and 3rd quartiles\n0.7; 1.4\n\n\nMin. and max.\n0.1; 22.1\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"4.3\", …, \"11.3\", \"12\", \"14.9\", \"17.3\", \"22.1\" (49 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#creatinine_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#creatinine_max",
    "title": "data",
    "section": "creatinine_max",
    "text": "creatinine_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n2 (0.04 %)\n\n\nNumber of unique values\n122\n\n\nMedian\n1.2\n\n\n1st and 3rd quartiles\n0.9; 1.9\n\n\nMin. and max.\n0.1; 27.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\", …, \"16\", \"17\", \"20.6\", \"27.5\", \"27.8\" (45 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#chloride_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#chloride_min",
    "title": "data",
    "section": "chloride_min",
    "text": "chloride_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1 (0.02 %)\n\n\nNumber of unique values\n66\n\n\nMedian\n102\n\n\n1st and 3rd quartiles\n98; 106\n\n\nMin. and max.\n58; 139\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"58\", \"61\", \"62\", \"64\", \"65\", …, \"127\", \"128\", \"133\", \"134\", \"139\" (23 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#chloride_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#chloride_max",
    "title": "data",
    "section": "chloride_max",
    "text": "chloride_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1 (0.02 %)\n\n\nNumber of unique values\n64\n\n\nMedian\n108\n\n\n1st and 3rd quartiles\n104; 112\n\n\nMin. and max.\n67; 155\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"67\", \"80\", \"81\", \"82\", \"84\", …, \"139\", \"140\", \"141\", \"144\", \"155\" (21 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#glucose_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#glucose_min",
    "title": "data",
    "section": "glucose_min",
    "text": "glucose_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n257\n\n\nMedian\n106\n\n\n1st and 3rd quartiles\n90; 127\n\n\nMin. and max.\n15; 425\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"15\", \"19\", \"20\", \"21\", \"22\", …, \"353\", \"355\", \"359\", \"424\", \"425\" (91 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#glucose_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#glucose_max",
    "title": "data",
    "section": "glucose_max",
    "text": "glucose_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n473\n\n\nMedian\n161\n\n\n1st and 3rd quartiles\n128; 211\n\n\nMin. and max.\n38; 2275\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"38\", \"40\", \"56\", \"57\", \"58\", …, \"1155\", \"1208\", \"1317\", \"1689\", \"2275\" (107 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#hematocrit_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#hematocrit_min",
    "title": "data",
    "section": "hematocrit_min",
    "text": "hematocrit_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n3 (0.07 %)\n\n\nNumber of unique values\n340\n\n\nMedian\n29.5\n\n\n1st and 3rd quartiles\n25.3; 34\n\n\nMin. and max.\n9; 55.9\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"9\", \"9.1\", \"9.9\", \"10.6\", \"11\", …, \"52.8\", \"53.3\", \"54\", \"54.5\", \"55.9\" (18 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#hematocrit_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#hematocrit_max",
    "title": "data",
    "section": "hematocrit_max",
    "text": "hematocrit_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n3 (0.07 %)\n\n\nNumber of unique values\n330\n\n\nMedian\n35.6\n\n\n1st and 3rd quartiles\n31.6; 40\n\n\nMin. and max.\n16.4; 61\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"16.4\", \"18.1\", \"19.1\", \"20.3\", \"20.6\", …, \"59.1\", \"60.4\", \"60.5\", \"60.9\", \"61\" (14 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#hemoglobin_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#hemoglobin_min",
    "title": "data",
    "section": "hemoglobin_min",
    "text": "hemoglobin_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n3 (0.07 %)\n\n\nNumber of unique values\n138\n\n\nMedian\n9.9\n\n\n1st and 3rd quartiles\n8.5; 11.4\n\n\nMin. and max.\n2.7; 18\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"2.7\", \"2.9\", \"3\", \"3.2\", \"3.5\", …, \"16.6\", \"17.2\", \"17.4\", \"17.5\", \"18\" (14 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#hemoglobin_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#hemoglobin_max",
    "title": "data",
    "section": "hemoglobin_max",
    "text": "hemoglobin_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n3 (0.07 %)\n\n\nNumber of unique values\n129\n\n\nMedian\n11.85\n\n\n1st and 3rd quartiles\n10.4; 13.4\n\n\nMin. and max.\n4.8; 19.6\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"4.8\", \"6\", \"6.3\", \"6.4\", \"18.5\", …, \"19.1\", \"19.3\", \"19.4\", \"19.5\", \"19.6\" (1 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#lactate_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#lactate_min",
    "title": "data",
    "section": "lactate_min",
    "text": "lactate_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n97\n\n\nMedian\n1.4\n\n\n1st and 3rd quartiles\n1; 2\n\n\nMin. and max.\n0.3; 20.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.3\", \"0.4\", \"5.4\", \"5.5\", \"5.6\", …, \"12.1\", \"12.6\", \"13.2\", \"15.6\", \"20.3\" (37 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#lactate_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#lactate_max",
    "title": "data",
    "section": "lactate_max",
    "text": "lactate_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n165\n\n\nMedian\n2.2\n\n\n1st and 3rd quartiles\n1.5; 3.5\n\n\nMin. and max.\n0.3; 32\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.3\", \"0.4\", \"0.5\", \"0.6\", \"0.7\", …, \"21.2\", \"21.4\", \"22.4\", \"23.5\", \"32\" (40 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#lactate_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#lactate_mean",
    "title": "data",
    "section": "lactate_mean",
    "text": "lactate_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n209\n\n\nMedian\n1.9\n\n\n1st and 3rd quartiles\n1.35; 2.75\n\n\nMin. and max.\n0.3; 20.85\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.3\", \"0.4\", \"0.45\", \"0.5\", \"0.55\", …, \"15.35\", \"15.55\", \"16.8\", \"19.55\", \"20.85\" (58 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#platelet_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#platelet_min",
    "title": "data",
    "section": "platelet_min",
    "text": "platelet_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n6 (0.13 %)\n\n\nNumber of unique values\n527\n\n\nMedian\n178\n\n\n1st and 3rd quartiles\n122; 246\n\n\nMin. and max.\n5; 1297\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"5\", \"6\", \"7\", \"8\", \"9\", …, \"951\", \"961\", \"996\", \"1039\", \"1297\" (57 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#platelet_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#platelet_max",
    "title": "data",
    "section": "platelet_max",
    "text": "platelet_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n6 (0.13 %)\n\n\nNumber of unique values\n608\n\n\nMedian\n224\n\n\n1st and 3rd quartiles\n159; 303\n\n\nMin. and max.\n16; 1775\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"16\", \"18\", \"19\", \"20\", \"21\", …, \"1081\", \"1163\", \"1308\", \"1418\", \"1775\" (73 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#potassium_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#potassium_min",
    "title": "data",
    "section": "potassium_min",
    "text": "potassium_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n51\n\n\nMedian\n3.7\n\n\n1st and 3rd quartiles\n3.4; 4.1\n\n\nMin. and max.\n0.9; 6.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.9\", \"1.2\", \"1.4\", \"1.5\", \"1.7\", …, \"5.9\", \"6\", \"6.2\", \"6.4\", \"6.8\" (13 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#potassium_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#potassium_max",
    "title": "data",
    "section": "potassium_max",
    "text": "potassium_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n73\n\n\nMedian\n4.5\n\n\n1st and 3rd quartiles\n4.1; 5.1\n\n\nMin. and max.\n2.3; 13.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"2.3\", \"2.8\", \"2.9\", \"3\", \"3.1\", …, \"9.7\", \"9.8\", \"9.9\", \"10.2\", \"13.5\" (17 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#inr_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#inr_min",
    "title": "data",
    "section": "inr_min",
    "text": "inr_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n270 (5.92 %)\n\n\nNumber of unique values\n68\n\n\nMedian\n1.2\n\n\n1st and 3rd quartiles\n1.1; 1.4\n\n\nMin. and max.\n0.6; 14.6\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.6\", \"0.7\", \"0.8\", \"0.9\", \"2.7\", …, \"7.1\", \"7.7\", \"12\", \"13\", \"14.6\" (38 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#inr_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#inr_max",
    "title": "data",
    "section": "inr_max",
    "text": "inr_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n270 (5.92 %)\n\n\nNumber of unique values\n98\n\n\nMedian\n1.3\n\n\n1st and 3rd quartiles\n1.2; 1.7\n\n\nMin. and max.\n0.6; 24\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.6\", \"0.8\", \"0.9\", \"1\", \"5.1\", …, \"18.8\", \"21\", \"21.1\", \"21.5\", \"24\" (47 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sodium_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sodium_min",
    "title": "data",
    "section": "sodium_min",
    "text": "sodium_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n61\n\n\nMedian\n136\n\n\n1st and 3rd quartiles\n133; 139\n\n\nMin. and max.\n74; 166\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"74\", \"95\", \"103\", \"104\", \"106\", …, \"160\", \"162\", \"163\", \"165\", \"166\" (26 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sodium_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sodium_max",
    "title": "data",
    "section": "sodium_max",
    "text": "sodium_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n63\n\n\nMedian\n140\n\n\n1st and 3rd quartiles\n138; 143\n\n\nMin. and max.\n108; 182\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"108\", \"113\", \"114\", \"116\", \"117\", …, \"173\", \"174\", \"175\", \"180\", \"182\" (33 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#bun_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#bun_min",
    "title": "data",
    "section": "bun_min",
    "text": "bun_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n2 (0.04 %)\n\n\nNumber of unique values\n139\n\n\nMedian\n19\n\n\n1st and 3rd quartiles\n13; 32\n\n\nMin. and max.\n1; 182\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"1\", \"2\", \"3\", \"4\", \"5\", …, \"152\", \"155\", \"160\", \"181\", \"182\" (16 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#bun_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#bun_max",
    "title": "data",
    "section": "bun_max",
    "text": "bun_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n2 (0.04 %)\n\n\nNumber of unique values\n156\n\n\nMedian\n24\n\n\n1st and 3rd quartiles\n16; 39\n\n\nMin. and max.\n3; 261\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"3\", \"4\", \"5\", \"6\", \"7\", …, \"212\", \"218\", \"229\", \"251\", \"261\" (13 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#bun_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#bun_mean",
    "title": "data",
    "section": "bun_mean",
    "text": "bun_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n256\n\n\nMedian\n21.5\n\n\n1st and 3rd quartiles\n14.5; 36\n\n\nMin. and max.\n0; 216.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0\", \"2\", \"3\", \"3.5\", \"4\", …, \"172\", \"178.5\", \"194.5\", \"206.5\", \"216.5\" (19 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#wbc_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#wbc_min",
    "title": "data",
    "section": "wbc_min",
    "text": "wbc_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n5 (0.11 %)\n\n\nNumber of unique values\n348\n\n\nMedian\n9.6\n\n\n1st and 3rd quartiles\n6.6; 13.4\n\n\nMin. and max.\n0.1; 336.7\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\", …, \"86\", \"87.8\", \"181.4\", \"243.6\", \"336.7\" (66 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#wbc_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#wbc_max",
    "title": "data",
    "section": "wbc_max",
    "text": "wbc_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n5 (0.11 %)\n\n\nNumber of unique values\n435\n\n\nMedian\n13.6\n\n\n1st and 3rd quartiles\n9.7; 18.5\n\n\nMin. and max.\n0.1; 471.7\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\", …, \"126.2\", \"183\", \"325.7\", \"462.6\", \"471.7\" (78 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#wbc_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#wbc_mean",
    "title": "data",
    "section": "wbc_mean",
    "text": "wbc_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n658\n\n\nMedian\n11.75\n\n\n1st and 3rd quartiles\n8.4; 15.85\n\n\nMin. and max.\n0; 404.2\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"0\", \"0.1\", \"0.15\", \"0.2\", \"0.25\", …, \"106.1\", \"182.2\", \"206.75\", \"353.1\", \"404.2\" (94 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#heartrate_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#heartrate_min",
    "title": "data",
    "section": "heartrate_min",
    "text": "heartrate_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n114\n\n\nMedian\n72\n\n\n1st and 3rd quartiles\n62; 84\n\n\nMin. and max.\n1; 134\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"1\", \"3\", \"4\", \"5\", \"8\", …, \"37\", \"127\", \"129\", \"130\", \"134\" (17 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#heartrate_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#heartrate_max",
    "title": "data",
    "section": "heartrate_max",
    "text": "heartrate_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n131\n\n\nMedian\n106\n\n\n1st and 3rd quartiles\n92; 121\n\n\nMin. and max.\n41; 223\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"41\", \"49\", \"50\", \"53\", \"55\", …, \"190\", \"194\", \"213\", \"215\", \"223\" (9 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#heartrate_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#heartrate_mean",
    "title": "data",
    "section": "heartrate_mean",
    "text": "heartrate_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n3902\n\n\nMedian\n87.78\n\n\n1st and 3rd quartiles\n76.02; 99.75\n\n\nMin. and max.\n35.78; 154.53\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"35.78\", \"36\", \"37.62\", \"138.48\", \"138.71\", …, \"139.14\", \"139.48\", \"149.12\", \"152.22\", \"154.53\" (1 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sysbp_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sysbp_min",
    "title": "data",
    "section": "sysbp_min",
    "text": "sysbp_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n8 (0.18 %)\n\n\nNumber of unique values\n134\n\n\nMedian\n88\n\n\n1st and 3rd quartiles\n79; 98\n\n\nMin. and max.\n5; 168\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"5\", \"10\", \"18\", \"22\", \"23\", …, \"151\", \"158\", \"161\", \"162\", \"168\" (49 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sysbp_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sysbp_max",
    "title": "data",
    "section": "sysbp_max",
    "text": "sysbp_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n8 (0.18 %)\n\n\nNumber of unique values\n151\n\n\nMedian\n146\n\n\n1st and 3rd quartiles\n132; 162\n\n\nMin. and max.\n83; 311\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"83\", \"89\", \"90\", \"91\", \"92\", …, \"252\", \"254\", \"265\", \"273\", \"311\" (21 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#sysbp_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#sysbp_mean",
    "title": "data",
    "section": "sysbp_mean",
    "text": "sysbp_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n8 (0.18 %)\n\n\nNumber of unique values\n3867\n\n\nMedian\n113.61\n\n\n1st and 3rd quartiles\n104.89; 125.63\n\n\nMin. and max.\n69.88; 195.34\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"69.88\", \"71.68\", \"71.73\", \"73.12\", \"75.44\", …, \"180.83\", \"181.96\", \"182.48\", \"188.29\", \"195.34\" (85 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#diasbp_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#diasbp_min",
    "title": "data",
    "section": "diasbp_min",
    "text": "diasbp_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n8 (0.18 %)\n\n\nNumber of unique values\n80\n\n\nMedian\n44\n\n\n1st and 3rd quartiles\n37; 51\n\n\nMin. and max.\n5; 114\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"5\", \"6\", \"11\", \"12\", \"13\", …, \"85\", \"88\", \"92\", \"93\", \"114\" (13 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#diasbp_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#diasbp_max",
    "title": "data",
    "section": "diasbp_max",
    "text": "diasbp_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n8 (0.18 %)\n\n\nNumber of unique values\n136\n\n\nMedian\n83\n\n\n1st and 3rd quartiles\n73; 96\n\n\nMin. and max.\n35; 197\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"35\", \"41\", \"44\", \"46\", \"48\", …, \"183\", \"185\", \"187\", \"196\", \"197\" (30 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#diasbp_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#diasbp_mean",
    "title": "data",
    "section": "diasbp_mean",
    "text": "diasbp_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n8 (0.18 %)\n\n\nNumber of unique values\n3621\n\n\nMedian\n60.3\n\n\n1st and 3rd quartiles\n54.09; 67\n\n\nMin. and max.\n26.33; 129.57\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"26.33\", \"28.4\", \"32.48\", \"32.94\", \"33.43\", …, \"103.52\", \"104.29\", \"105.46\", \"107.43\", \"129.57\" (67 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#meanbp_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#meanbp_min",
    "title": "data",
    "section": "meanbp_min",
    "text": "meanbp_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n103\n\n\nMedian\n56\n\n\n1st and 3rd quartiles\n49; 63\n\n\nMin. and max.\n1; 113\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"1\", \"2\", \"3\", \"4\", \"5\", …, \"101\", \"102\", \"104\", \"109\", \"113\" (37 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#meanbp_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#meanbp_max",
    "title": "data",
    "section": "meanbp_max",
    "text": "meanbp_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n197\n\n\nMedian\n101\n\n\n1st and 3rd quartiles\n90; 114\n\n\nMin. and max.\n56; 298\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"56\", \"58\", \"60\", \"62\", \"63\", …, \"294\", \"295\", \"296\", \"297\", \"298\" (89 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#meanbp_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#meanbp_mean",
    "title": "data",
    "section": "meanbp_mean",
    "text": "meanbp_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n3677\n\n\nMedian\n75\n\n\n1st and 3rd quartiles\n69.04; 82.41\n\n\nMin. and max.\n45.6; 133.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"45.6\", \"46.97\", \"48.36\", \"48.71\", \"49.83\", …, \"122.48\", \"123.21\", \"124.96\", \"131.5\", \"133.16\" (71 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#resprate_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#resprate_min",
    "title": "data",
    "section": "resprate_min",
    "text": "resprate_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1 (0.02 %)\n\n\nNumber of unique values\n30\n\n\nMedian\n13\n\n\n1st and 3rd quartiles\n10; 15\n\n\nMin. and max.\n1; 32\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"1\", \"2\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"30\", \"32\"."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#resprate_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#resprate_max",
    "title": "data",
    "section": "resprate_max",
    "text": "resprate_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1 (0.02 %)\n\n\nNumber of unique values\n53\n\n\nMedian\n27\n\n\n1st and 3rd quartiles\n24; 32\n\n\nMin. and max.\n13; 69\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"13\", \"14\", \"15\", \"16\", \"17\", …, \"61\", \"62\", \"64\", \"68\", \"69\" (10 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#resprate_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#resprate_mean",
    "title": "data",
    "section": "resprate_mean",
    "text": "resprate_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1 (0.02 %)\n\n\nNumber of unique values\n3110\n\n\nMedian\n19.26\n\n\n1st and 3rd quartiles\n16.79; 22.4\n\n\nMin. and max.\n9.54; 40.58\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"9.54\", \"9.78\", \"10.17\", \"10.5\", \"10.79\", …, \"38.44\", \"38.96\", \"40.16\", \"40.37\", \"40.58\" (20 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#tempc_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#tempc_min",
    "title": "data",
    "section": "tempc_min",
    "text": "tempc_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n103 (2.26 %)\n\n\nNumber of unique values\n167\n\n\nMedian\n36.11\n\n\n1st and 3rd quartiles\n35.61; 36.61\n\n\nMin. and max.\n15; 39.6\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"15\", \"26.67\", \"29.44\", \"29.9\", \"30.56\", …, \"38.7\", \"38.72\", \"38.94\", \"39.2\", \"39.6\" (53 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#tempc_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#tempc_max",
    "title": "data",
    "section": "tempc_max",
    "text": "tempc_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n103 (2.26 %)\n\n\nNumber of unique values\n164\n\n\nMedian\n37.5\n\n\n1st and 3rd quartiles\n37; 38.11\n\n\nMin. and max.\n31.6; 42\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"31.6\", \"32.2\", \"32.4\", \"32.61\", \"32.7\", …, \"40.9\", \"40.94\", \"41.11\", \"41.6\", \"42\" (39 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#tempc_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#tempc_mean",
    "title": "data",
    "section": "tempc_mean",
    "text": "tempc_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n103 (2.26 %)\n\n\nNumber of unique values\n1692\n\n\nMedian\n36.82\n\n\n1st and 3rd quartiles\n36.41; 37.31\n\n\nMin. and max.\n31.6; 40.1\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"31.6\", \"32.02\", \"32.1\", \"32.2\", \"32.35\", …, \"39.36\", \"39.45\", \"39.56\", \"39.71\", \"40.1\" (84 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#spo2_min",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#spo2_min",
    "title": "data",
    "section": "spo2_min",
    "text": "spo2_min\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1 (0.02 %)\n\n\nNumber of unique values\n71\n\n\nMedian\n93\n\n\n1st and 3rd quartiles\n90; 95\n\n\nMin. and max.\n1; 100\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"1\", \"5\", \"11\", \"13\", \"18\", …, \"74\", \"75\", \"76\", \"99\", \"100\" (39 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#spo2_max",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#spo2_max",
    "title": "data",
    "section": "spo2_max",
    "text": "spo2_max\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1 (0.02 %)\n\n\nNumber of unique values\n12\n\n\nMedian\n100\n\n\n1st and 3rd quartiles\n100; 100\n\n\nMin. and max.\n89; 100\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"89\", \"90\", \"91\", \"92\", \"93\", …, \"95\", \"96\", \"97\", \"98\", \"99\" (1 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#spo2_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#spo2_mean",
    "title": "data",
    "section": "spo2_mean",
    "text": "spo2_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1 (0.02 %)\n\n\nNumber of unique values\n2015\n\n\nMedian\n97.26\n\n\n1st and 3rd quartiles\n95.8; 98.6\n\n\nMin. and max.\n55.87; 100\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"55.87\", \"71.85\", \"76.16\", \"76.38\", \"78.08\", …, \"89.81\", \"89.96\", \"90\", \"90.07\", \"90.13\" (55 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#glucose_min1",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#glucose_min1",
    "title": "data",
    "section": "glucose_min1",
    "text": "glucose_min1\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n30 (0.66 %)\n\n\nNumber of unique values\n245\n\n\nMedian\n106\n\n\n1st and 3rd quartiles\n89; 127\n\n\nMin. and max.\n12; 480\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"12\", \"15\", \"16\", \"19\", \"20\", …, \"356\", \"357\", \"374\", \"424\", \"480\" (80 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#glucose_max1",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#glucose_max1",
    "title": "data",
    "section": "glucose_max1",
    "text": "glucose_max1\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n30 (0.66 %)\n\n\nNumber of unique values\n454\n\n\nMedian\n166\n\n\n1st and 3rd quartiles\n130; 217\n\n\nMin. and max.\n57; 999999\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following suspected missing value codes enter as regular values: \"999999\".\nNote that the following possible outlier values were detected: \"57\", \"62\", \"64\", \"65\", \"66\", …, \"883\", \"900\", \"1016\", \"1038\", \"999999\" (62 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#glucose_mean",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#glucose_mean",
    "title": "data",
    "section": "glucose_mean",
    "text": "glucose_mean\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n30 (0.66 %)\n\n\nNumber of unique values\n2228\n\n\nMedian\n134.19\n\n\n1st and 3rd quartiles\n113; 165\n\n\nMin. and max.\n52.44; 142966.86\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"52.44\", \"57\", \"62\", \"62.75\", \"63.07\", …, \"479.26\", \"578.25\", \"657\", \"786.22\", \"142966.86\" (114 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#rrt",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#rrt",
    "title": "data",
    "section": "rrt",
    "text": "rrt\n\nNote that this variable is treated as a factor variable below, as it only takes a few unique values.\n\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n2\n\n\nMode\n“0”\n\n\nReference category\n0"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#subject_id",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#subject_id",
    "title": "data",
    "section": "subject_id",
    "text": "subject_id\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4559\n\n\nMedian\n68368\n\n\n1st and 3rd quartiles\n53111.5; 83771.5\n\n\nMin. and max.\n165; 99982\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"165\", \"266\", \"671\", \"5771\", \"7310\"."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#hadm_id102",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#hadm_id102",
    "title": "data",
    "section": "hadm_id…102",
    "text": "hadm_id…102\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4559\n\n\nMedian\n149643\n\n\n1st and 3rd quartiles\n125389; 175033\n\n\nMin. and max.\n100003; 199962"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#icustay_id103",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#icustay_id103",
    "title": "data",
    "section": "icustay_id…103",
    "text": "icustay_id…103\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n4559\n\n\nMedian\n251008\n\n\n1st and 3rd quartiles\n225575.5; 275526\n\n\nMin. and max.\n200075; 299998"
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#urineoutput",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#urineoutput",
    "title": "data",
    "section": "urineoutput",
    "text": "urineoutput\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n0 (0 %)\n\n\nNumber of unique values\n1794\n\n\nMedian\n1560\n\n\n1st and 3rd quartiles\n897.5; 2460\n\n\nMin. and max.\n0; 50515\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"6540\", \"6560\", \"6640\", \"6650\", \"6670\", …, \"9910\", \"10525\", \"11025\", \"12210\", \"50515\" (34 values omitted)."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#colloid_bolus",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#colloid_bolus",
    "title": "data",
    "section": "colloid_bolus",
    "text": "colloid_bolus\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n4050 (88.84 %)\n\n\nNumber of unique values\n13\n\n\nMedian\n500\n\n\n1st and 3rd quartiles\n250; 500\n\n\nMin. and max.\n150; 1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"550\", \"600\", \"900\", \"1000\"."
  },
  {
    "objectID": "MIMIC_ICU_Data/dataMaid_data.html#crystalloid_bolus",
    "href": "MIMIC_ICU_Data/dataMaid_data.html#crystalloid_bolus",
    "title": "data",
    "section": "crystalloid_bolus",
    "text": "crystalloid_bolus\n\n\n\n\n\n\n\n\n\nFeature\nResult\n\n\n\n\nVariable type\nnumeric\n\n\nNumber of missing obs.\n1194 (26.19 %)\n\n\nNumber of unique values\n96\n\n\nMedian\n500\n\n\n1st and 3rd quartiles\n500; 1000\n\n\nMin. and max.\n250; 11000\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the following possible outlier values were detected: \"250\", \"258\", \"267\", \"275\", \"276\", …, \"423\", \"433\", \"450\", \"467\", \"475\" (21 values omitted).\n\n\nReport generation information:\n\nCreated by: Could not determine from system (username: ecm20).\nReport creation time: Sun Oct 08 2023 10:44:20\nReport was run from directory: /home/ecm20/R/STA6257_Project_SVM/Josh Branch/STA6257_Project_SVM_Josh/Team Repo\ndataMaid v1.4.1 [Pkg: 2021-10-08 from CRAN (R 4.3.1)]\nR version 4.3.1 (2023-06-16).\nPlatform: x86_64-pc-linux-gnu (64-bit)(America/Chicago).\nFunction call: makeDataReport(data = data, output = \"html\", replace = TRUE)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "",
    "text": "Every day sensors and systems are capturing a virtual flood of data points and feeding those values into powerful artificial intelligence and machine learning systems to derive classifications and predict a myriad of outcomes. These systems help us do a broad spectrum of things from fraud detection to interactions with smart home systems. Given the number of data points that are present in the medical field, it is of no surprise that machine learning is increasingly being leveraged to elevate patient care.\nFor as long as most of us remember, our interactions with physicians have included the gathering of data points to help them detect illness and track progression of disease. Common data points are age, weight, height, temperature, blood pressure, list of any current symptoms and etc. Physicians then use their education and years of practice to provide diagnoses and help us live happy and healthy lives. But what if this was process could be supported with machine learning, and brought into a more critical care setting?\nWe can. Thanks to machine learning we can use data from the countless sensors and measurements taken by medical staff in Intensive Care Units (ICU) to predict the survivability of patients under care. This data can then help teams organize around certain cases to help ensure the best allotment of resources and highest attention to the most dire of cases. A method we will use in this report is through the construction of Support Vector Machines or SVM’s.\nSVM’s are a machine learning methodology that uses supervised learning based on historic cases to help train models that can then be used on other cases. Models created by Support Vector Machines are used to create clusters of 2 distinct groups of data based on the creation of a maximally-marginal hyperplane.(Han and Pei 2012). A maximally-marginal hyperplane can be explained as a line that can be drawn between two clusters that separates their members with the greatest distance between members of each cluster that are their nearest.\nWhile we did not find many cases of using support vector machines for ICU patient survival, the use of Support Vector Machines in medicine is not a novel approach. In “Using Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records” by Adeoye the author leveraged electronic medical records to help classify individuals with and without diabetes. Meanwhile, Zhou et al where able to use Support Vector Machines to predict the prognosis of severe, acute myocardial infarction with 92% accuracy from electronic medical records(Zhou 2023)\nWhile Support Vector Machines can be a great utility used to cluster and predict outcomes, their usage in Medicine is not without problems. One problem noted by Liu et al is that the sheer number of data points available can make it hard to find features (data points of importance) for use in model construction.(Liu June 2018) In fact Liu goes as far as describing the use of Principal Component Analysis to chose which features to include in model construction. Additionally, support vector machines work best when the data can be mapped linearly, however unlike other methodologies this is not a strict requirement. Should data not be easily linearly separable the use of a kernel function or kernel trick allows data to be mapped from one space to another for optimal construction of the maximally marginal hyperplane (Han and Pei 2012) (Mohan et al. 2020)"
  },
  {
    "objectID": "index.html#statistical-modeling",
    "href": "index.html#statistical-modeling",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\nGiven that Support Vector Machines operate on both linear and non-linear data, we must look at how a Support Vector Machine manipulates non-linear data to a linear space to perform classification. This operation is done via a kernel function or “kernel trick”. The general formula for a kernel function is as follows.\n\\[\nK(X_i, X_j) = \\Phi(X_i)\\Phi(X_j)  \n\\]\nWhere \\(X_i, X_j\\) is a tuple.\nOnce the data linear (and therefore linearly separable), we can define our maximally marginal hyperplane. A general formula for the hyperplane is as follows \\[\nW \\times X + b = 0\n\\]\nThis formula has 2 components of import. The first is a \\(W\\), a weight vector and \\(b\\) a scalar bias.\nSince \\(W\\) is a simple weighting vector it would be in the form of\n\\[\nW = \\{w_1, w_2, \\dots, w_n \\}\n\\]\nThe tuples that lie the closest to the margins of the maximal marginal hyperplane are the actual support vectors.\nUsing the formulas above, if our support vectors where at \\(y_i = 1\\) and \\(y_i = -1\\) our hyperplane margins would be defined as\n\\[\nH_1: W_0 + W_{1}X_{1} + W_{2}X_{2} + \\dots + W_{n}X_{n} \\ge 1\n\\]\nand\n\\[\nH_2: W_0 + W_{1}X_{1} + W_{2}X_{2} + \\dots + W_{n}X_{n} \\le -1\n\\]"
  },
  {
    "objectID": "index.html#model",
    "href": "index.html#model",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Model",
    "text": "Model\nIn order to forecast the possibility of patient death during their hospital stay, a support vector machine (SVM) model had to be developed and evaluated. A particular subset of the MIMIC dataset, a vast collection of ICU electronic health records, was used to train the algorithm. Age, gender, hospital expiration flag, mean heart rate, mean systolic blood pressure, mean respiration rate, mean temperature, mean white blood cell count, minimum platelet count, maximum creatinine, and mean lactate were among the variables included in the dataset.\nThe dataset included 4,591 ICU patients from a single center. The outcome was binary - died or survived during hospitalization. 741 patients (16%) died while 3,850 (84%) survived. The model aimed to predict this binary mortality outcome using demographics, vital signs, and lab results as predictors. An SVM with radial kernel was chosen due to its ability to handle binary classification problems.\nSeveral assumptions were made in the modeling approach. The dataset was assumed to be representative of the broader ICU population at the institution. It was also assumed that the selected predictors sufficiently captured factors associated with mortality risk.\nThe SVM model was optimized using a 10-fold cross-validation method, and its performance on the training dataset was evaluated. A thorough evaluation was carried out on a range of values for the cost parameter, which controls the ratio of overfitting to underfitting. For testing on the test set, the model with the best performance on the training set was selected. The dataset was split into training (80%) and test (20%) sets for model development and evaluation. Predictors included age, gender, heart rate, blood pressure, respiratory rate, temperature, white blood cell count, platelet count, creatinine, and lactate levels.\nOn the test dataset, the model showed an accuracy rate of 85.47% and a specificity rate of 86.13%. This shows that the model successfully identified patients who would survive their hospital stay in 86.13% of the cases, and it also obtained an accuracy rate of 85.47% in predicting patient mortality during their hospitalization.\nFurthermore, we calculated the following metrics for the model using the test dataset. It is found that the measurement’s sensitivity is 0.8187 and that the positive predictive value (PPV) is 0.8187 with a negative predictive value of 0.8345.\nHyperparameter tuning was performed on the training set over a range of cost values from 0.001 to 100. The optimal cost of 0.1 was selected based on maximizing accuracy.\nThe model aimed to predict this binary mortality outcome using demographics, vital signs, and lab results as predictors. An SVM with radial kernel was chosen due to its ability to handle binary classification problems.\nSeveral assumptions were made in the modeling approach. The dataset was assumed to be representative of the broader ICU population at the institution. It was also assumed that the selected predictors sufficiently captured factors associated with mortality risk.\nThe data was split 80/20 into training and test sets. The training set was used for tuning model hyperparameters and fitting the final model. The test set provided an unbiased evaluation of model performance.\nTuning was performed over a range of cost parameter values from 0.001 to 100 on the training data only. Optimization sought to maximize accuracy. The best cost of 0.1 was selected.\nThe measurements suggest that the model is capable of correctly identifying patients who do not have a probability of dying and reliably forecasting survival in the ICU. The study’s conclusions suggest that the support vector machine (SVM) model has a great deal of promise as a predictive tool for estimating the risk of patient death while they are in the hospital. When run on the test set, the model demonstrates a favorable degree of accuracy and specificity, demonstrating its ability to generalize to new data with effectiveness.\nIt can be inferred that our support vector machine (SVM) model demonstrated moderate accuracy in predicting hospital mortality. The predictive accuracy of the model was higher for patients who survived compared to those who did not. According to the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, the model can tell the difference between patients who will die and those who will not. The area under the receiver operating characteristic (ROC) curve was determined to be 0.764.\nOur investigation has a number of limitations. In the beginning, the model was trained and assessed using a relatively small dataset. Additionally, it should be mentioned that the dataset used in this research was only sourced from one hospital. As a result, there is a chance that patients from various institutions will not benefit from the model’s performance. Moreover, it’s critical to remember that not all attributes that might be involved in mortality prediction were included in our analysis.\nDespite these limitations, our study provides evidence that Support Vector Machine (SVM) models can be used to accurately predict survival in hospitalized patients with a high degree of precision. Further research is needed to validate the model’s performance on a larger and more diverse dataset and investigate the practical implications of using the model to predict patient survival.\nThe goal of applying the support vector machine (SVM) model is to improve patient care by applying it to various therapeutic settings. The following are some possible uses for the model, including to identify those who are more likely to die, allowing for the administration of more intensive medical measures. Also, we can create a mechanism to decide which patients are admitted to intensive care units and in what order. The goal of this research is to offer a thorough framework that will direct treatment decisions for patients who present with complicated medical issues.\nThis support vector machine (SVM) model shows a great deal of promise as a useful tool for improving patient care and predicting mortality. Further research is necessary to validate the model’s performance on a larger and more diverse dataset and investigate the practical implications of using the model to predict survival.\n\nData preparation\n\n\nCode\n# Create a subset of the entire data set to just include the rows we have determined are of interest\n# convert the gender field to an `is_male` flag. Also discretize the heart rate and age\nmodel_data &lt;- mimic_data %&gt;%\n  select(\n    icustay_id,\n    age,\n    gender,\n    hospital_expire_flag,\n    heartrate_mean,\n    sysbp_mean,\n    resprate_mean,\n    tempc_mean,\n    wbc_mean,\n    platelet_min,\n    creatinine_max,\n    lactate_mean\n  ) %&gt;%\n  mutate(\n    is_male = factor(case_when(gender == \"M\" ~ 1, TRUE ~ 0)),\n    age_range = factor(\n      case_when(\n        age &lt;= 18 ~ \"&lt;=18\",\n        age &gt; 18 & age &lt;= 40 ~ \"19 to 40\",\n        age &gt; 40 & age &lt;= 60 ~ \"41 to 60\",\n        age &gt; 60 ~ \"&gt;=61\"\n      )\n    ),\n    heart_rate = factor(\n      case_when(\n        heartrate_mean &lt; 60 ~ \"&lt;60\",\n        heartrate_mean &gt;= 61 & heartrate_mean &lt;= 80 ~ \"60 to 80\",\n        heartrate_mean &gt;= 81 & heartrate_mean &lt;= 100 ~ \"81 to 100\",\n        heartrate_mean &gt; 100 ~ \"above 100\"\n      )\n    ),\n    survival = factor(case_when(\n      hospital_expire_flag == 1 ~ \"DIED\", TRUE ~ \"SURVIVED\"\n    ))\n  ) %&gt;%\n  select(-gender,-heartrate_mean,-age,-hospital_expire_flag) %&gt;%\n  drop_na()\n\n\n# use z score standardization on our features to see what we get\nscaled_model_data &lt;- model_data %&gt;%\n  select(-icustay_id) %&gt;%\n  mutate_if(is.numeric, scale) %&gt;%\n  cbind(icustay_id = model_data$icustay_id)\n\n#create survival factor of levels for graphing\nsurvivalLevels &lt;- attributes(model_data$survival)$levels\n\n# set a random seed so that this is repeatable.\nset.seed(params$rand_seed)\n\n# our data are imbalanced such that the patients tend to survive (hospital expire flag = 0), to resolve this issue we will downsample \n# the surviving patients and construct a training set based on an 80% selection of all of the survival == \"DIED\"\n# then randomly select and equal number of observations where survival == \"DIED\"\namount_to_sample = floor(sum(scaled_model_data$survival == \"DIED\") * 0.95)\ntrain &lt;-\n  scaled_model_data %&gt;% group_by(survival) %&gt;% sample_n(size = amount_to_sample) %&gt;% ungroup()\ntest &lt;- scaled_model_data %&gt;% anti_join(train, by = \"icustay_id\")\n\n\n\n\nNu Classification\n\n\nCode\nset.seed(params$rand_seed)\nnuModel &lt;- e1071::svm(survival ~ . - icustay_id,\n                      type=\"nu-classification\",\n                      kernel=\"radial\",\n                      data=train,\n                      probability=TRUE,\n                      gamma=params$nu_gamma,\n                      nu=params$nu_nu,\n                      scale=TRUE)\n\nnuPredictions &lt;- predict(nuModel, test, probability=TRUE)\nnuProbabilities &lt;- attr(nuPredictions, \"probabilities\")[,1]\nnuROC &lt;- pROC::roc(as.factor(test$survival), nuProbabilities)\nnuConfusionMatrix &lt;- confusionMatrix(nuPredictions, test$survival)\n\n#to appropriately build the roc graph, buid a label order based on survivalLevels and the positive case from the confusion matrix\nnuOrdering &lt;- c(survivalLevels[which(survivalLevels != nuConfusionMatrix$positive)], nuConfusionMatrix$positive)\n\nnuROCRPredictions &lt;- ROCR::prediction(nuProbabilities, test$survival, label.ordering = nuOrdering)\nnuPerformance &lt;- ROCR::performance(nuROCRPredictions, \"tpr\", \"fpr\")\n\nfourfoldplot(as.table(nuConfusionMatrix), color=c(\"navy\", \"lightblue\"), main=\"nu-classification Confusion Matrix\")\n\n\n\n\n\nCode\nplot(nuPerformance, colorize = FALSE, main=\"ROC Curve for nu-classification\")\nabline(a=0,b=1)\nmtext(paste(\"AUC =\", round(as.numeric(nuROC$auc), 4)))\n\n\n\n\n\n\n\nC Classification\n\n\nCode\nset.seed(params$rand_seed)\n\ncModel &lt;- e1071::svm(survival ~ . - icustay_id,\n                      type=\"C-classification\",\n                      kernel=\"radial\",\n                      data=train,\n                      probability=TRUE,\n                      cost=params$c_cost,\n                      gamma=params$c_gamma,\n                      scale=TRUE)\n\ncPredictions &lt;- predict(cModel, test, probability=TRUE)\ncProbabilities &lt;- attr(cPredictions, \"probabilities\")[,1]\ncROC &lt;- pROC::roc(as.factor(test$survival), cProbabilities)\ncConfusionMatrix &lt;- confusionMatrix(cPredictions, test$survival)\n\n#to appropriately build the roc graph, buid a label order based on survivalLevels and the positive case from the confusion matrix\ncOrdering &lt;- c(survivalLevels[which(survivalLevels != cConfusionMatrix$positive)], cConfusionMatrix$positive)\n\ncROCRPredictions &lt;- ROCR::prediction(cProbabilities, test$survival, label.ordering = cOrdering)\ncPerformance &lt;- ROCR::performance(cROCRPredictions, \"tpr\", \"fpr\")\n\nfourfoldplot(as.table(cConfusionMatrix), color=c(\"navy\", \"lightblue\"), main=\"C-classification Confusion Matrix\")\n\n\n\n\n\nCode\nplot(cPerformance, colorize = FALSE, main=\"ROC Curve for C-Classification\")\nabline(a=0,b=1)\nmtext(paste(\"AUC =\", round(as.numeric(cROC$auc), 4)))\n\n\n\n\n\n\n\nChallenger Modeling\nA challenger variant was constructed by reducing the number of features. The objective was to assess whether a more modest model could attain comparable performance.\nAge, gender, heart rate, systolic blood pressure, respiratory rate, and lactate level were utilized as predictors by the challenger. Once more, an SVM with a radial kernel was implemented for classification.\nThe evaluation and selection of models adhered to the identical procedure as the initial model. The hyperparameters were adjusted across a range of cost values using 10-fold cross-validation on the training set. On the test set, the model whose validation performance was the strongest was chosen and assessed.\nThe challenger model achieved marginally inferior accuracy and specificity scores of 83.45%. Nevertheless, its reduced feature count enhances interpretability. The comparable performance achieved with fewer inputs gives us the minimum number of predictors required.\n\n\nCode\nset.seed(params$rand_seed)\nchallnger_test &lt;- test %&gt;% select(sysbp_mean, resprate_mean, tempc_mean, platelet_min, is_male, age_range, heart_rate, survival, icustay_id)\nchallnger_train &lt;- train %&gt;% select(sysbp_mean, resprate_mean, tempc_mean, platelet_min, is_male, age_range, heart_rate, survival, icustay_id)\n\n\n\nNu Classification\n\n\nCode\nset.seed(params$rand_seed)\nnuChallengerModel &lt;- e1071::svm(survival ~ . - icustay_id,\n                      type=\"nu-classification\",\n                      kernel=\"radial\",\n                      data=challnger_train,\n                      probability=TRUE,\n                      gamma=params$nu_challenger_gamma,\n                      nu=params$nu_challenger_nu,\n                      scale=TRUE)\n\nnuChallengerPredictions &lt;- predict(nuChallengerModel, challnger_test, probability=TRUE)\nnuChallengerProbabilities &lt;- attr(nuChallengerPredictions, \"probabilities\")[,1]\nnuChallengerROC &lt;- pROC::roc(as.factor(challnger_test$survival), nuChallengerProbabilities)\nnuChallengerConfusionMatrix &lt;- confusionMatrix(nuChallengerPredictions, challnger_test$survival)\n\n#to appropriately build the roc graph, buid a label order based on survivalLevels and the positive case from the confusion matrix\nnuChallengerOrdering &lt;- c(survivalLevels[which(survivalLevels != nuChallengerConfusionMatrix$positive)], nuChallengerConfusionMatrix$positive)\n\nnuChallengerROCRPredictions &lt;- ROCR::prediction(nuChallengerProbabilities, challnger_test$survival, label.ordering = nuOrdering)\nnuChallengerPerformance &lt;- ROCR::performance(nuChallengerROCRPredictions, \"tpr\", \"fpr\")\n\nfourfoldplot(as.table(nuChallengerConfusionMatrix), color=c(\"navy\", \"lightblue\"), main=\"nu-classification Challenger Confusion Matrix\")\n\n\n\n\n\nCode\nplot(nuChallengerPerformance, colorize = FALSE, main=\"ROC Curve for nu-classification (Challenger)\")\nabline(a=0,b=1)\nmtext(paste(\"AUC =\", round(as.numeric(nuChallengerROC$auc), 4)))\n\n\n\n\n\n\n\n\nC Classification\n\n\nCode\nset.seed(params$rand_seed)\ncChallengerModel &lt;- e1071::svm(survival ~ . - icustay_id,\n                      type=\"C-classification\",\n                      kernel=\"radial\",\n                      data=challnger_train,\n                      probability=TRUE,\n                      cost=params$c_challenger_cost,\n                      gamma=params$c_challenger_gamma,\n                      scale=TRUE)\n\ncChallengerPredictions &lt;- predict(cChallengerModel, challnger_test, probability=TRUE)\ncChallengerProbabillities &lt;- attr(cChallengerPredictions, \"probabilities\")[,1]\ncChallengerROC &lt;- pROC::roc(as.factor(challnger_test$survival), cChallengerProbabillities)\ncChallengerConfusionMatrix &lt;- confusionMatrix(cChallengerPredictions, challnger_test$survival)\n\n#to appropriately build the roc graph, buid a label order based on survivalLevels and the positive case from the confusion matrix\ncChallengerOrdering &lt;- c(survivalLevels[which(survivalLevels != cChallengerConfusionMatrix$positive)], cChallengerConfusionMatrix$positive)\n\ncChallengerROCRPredictions &lt;- ROCR::prediction(cChallengerProbabillities, challnger_test$survival, label.ordering = cChallengerOrdering)\ncChallengerPerformance &lt;- ROCR::performance(cChallengerROCRPredictions, \"tpr\", \"fpr\")\n\nfourfoldplot(as.table(cChallengerConfusionMatrix), color=c(\"navy\", \"lightblue\"), main=\"C-classification Confusion Matrix (Challenger)\")\n\n\n\n\n\nCode\nplot(cChallengerPerformance, colorize = FALSE, main=\"ROC Curve for C-Classification (Challenger)\")\nabline(a=0,b=1)\nmtext(paste(\"AUC =\", round(as.numeric(cChallengerROC$auc), 4)))"
  },
  {
    "objectID": "index.html#data-and-visualization",
    "href": "index.html#data-and-visualization",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Data and Visualization",
    "text": "Data and Visualization\n\n\nCode\nlibrary(gtsummary)\n\nmimic_data %&gt;%\nmutate(\n  gender = case_when(gender == \"M\" ~ \"male\",\n                     gender == \"F\" ~ \"female\",\n                     TRUE ~ gender),\n  survived = case_when(hospital_expire_flag == 1 ~ \"Died\",\n                     hospital_expire_flag == 0 ~ \"Survived\")\n) %&gt;%  \nselect(\n  age, \n  gender, \n  survived, \n  heartrate_mean,\n  sysbp_mean,\n  resprate_mean,\n  tempc_mean,\n  wbc_mean,\n  platelet_min,\n  creatinine_max,\n  lactate_mean\n) %&gt;%\n  tbl_summary(\n    type = list(age ~ 'continuous2',\n    gender ~ 'categorical', resprate_mean ~ 'continuous2',\n    heartrate_mean ~ 'continuous2',\n    tempc_mean ~ 'continuous2',\n    wbc_mean ~ 'continuous2',\n    platelet_min ~ 'continuous2',\n    creatinine_max ~ 'continuous2',\n    lactate_mean ~ 'continuous2',\n    sysbp_mean ~ 'continuous2'),\n    label = list(\n      age ~ \"Patient Age\",\n      gender ~ \"Patient Sex\",\n      heartrate_mean ~ \"Heart Rate\",\n      sysbp_mean ~ \"Systolic Blood Pressure\",\n      resprate_mean ~ \"Respiration Rate\",\n      tempc_mean ~ \"Body Temperature (c)\",\n      wbc_mean ~ \"White Blood Cell Count\",\n      platelet_min ~\"Platelet Count\",\n      creatinine_max ~\"Creatinine Level\",\n      lactate_mean ~\"Lactate Level\"\n       ),\n      statistic = all_continuous() ~ c(\"{median}({p25}, {p75})\", \"{min}, {max}\"),\n      by = survived \n  ) %&gt;%\n  add_overall(last = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  italicize_levels() %&gt;%   as_gt() %&gt;%\n  gt_theme_dark() %&gt;%\n  tab_options(\n    table.background.color = \"#d8e4ea\",\n    column_labels.background.color=\"#5092c2\",\n    table.align = \"left\"\n  )\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Died, N = 7411\n      Survived, N = 3,8181\n      Overall, N = 4,5591\n    \n  \n  \n    Patient Age\n\n\n\n        Median(IQR)\n73(60, 83)\n65(53, 78)\n67(54, 80)\n        Range\n17, 91\n17, 91\n17, 91\n    Patient Sex\n\n\n\n        female\n339 (46%)\n1,639 (43%)\n1,978 (43%)\n        male\n402 (54%)\n2,179 (57%)\n2,581 (57%)\n    Heart Rate\n\n\n\n        Median(IQR)\n92(77, 106)\n87(76, 99)\n88(76, 100)\n        Range\n47, 155\n36, 139\n36, 155\n    Systolic Blood Pressure\n\n\n\n        Median(IQR)\n108(100, 120)\n115(106, 126)\n114(105, 126)\n        Range\n70, 175\n76, 195\n70, 195\n        Unknown\n2\n6\n8\n    Respiration Rate\n\n\n\n        Median(IQR)\n21.5(18.4, 24.9)\n18.9(16.6, 21.9)\n19.3(16.8, 22.4)\n        Range\n11.3, 40.6\n9.5, 40.4\n9.5, 40.6\n        Unknown\n0\n1\n1\n    Body Temperature (c)\n\n\n\n        Median(IQR)\n36.62(36.11, 37.19)\n36.87(36.47, 37.32)\n36.82(36.41, 37.31)\n        Range\n31.60, 39.71\n32.61, 40.10\n31.60, 40.10\n        Unknown\n20\n83\n103\n    White Blood Cell Count\n\n\n\n        Median(IQR)\n13(9, 18)\n12(8, 15)\n12(8, 16)\n        Range\n0, 404\n0, 207\n0, 404\n    Platelet Count\n\n\n\n        Median(IQR)\n166(95, 253)\n180(126, 245)\n178(122, 246)\n        Range\n8, 951\n5, 1,297\n5, 1,297\n        Unknown\n1\n5\n6\n    Creatinine Level\n\n\n\n        Median(IQR)\n1.60(1.00, 2.60)\n1.10(0.80, 1.70)\n1.20(0.90, 1.90)\n        Range\n0.20, 14.40\n0.10, 27.80\n0.10, 27.80\n        Unknown\n1\n1\n2\n    Lactate Level\n\n\n\n        Median(IQR)\n2.55(1.70, 4.50)\n1.80(1.30, 2.55)\n1.90(1.35, 2.75)\n        Range\n0.40, 20.85\n0.30, 16.80\n0.30, 20.85\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\nPatient Demographics\n\nPatient Age\n\n\nCode\nggplot(mimic_data, aes(x=age))+ \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")\n\n\n\n\n\n\n\nPatient Sex\n\n\nCode\nmimic_data &lt;- mimic_data %&gt;% mutate(\n  gender = case_when(gender == \"M\" ~ \"male\",\n                     gender == \"F\" ~ \"female\",\n                     TRUE ~ gender)\n)\n\npatient_sex_viz &lt;- mimic_data %&gt;%\n  group_by(gender) %&gt;%\n  summarise(N = n()) %&gt;%\n  mutate(\n    gender = as.factor(gender),\n    pos = cumsum(N) - N/2,\n    label = paste(N,  \" \", gender, \"\\npatients\\n(\", 100*round(N/sum(N), 2), \"%)\", sep=\"\")\n  )\n\nggplot(patient_sex_viz, aes(x = \"\", y = N, fill = gender)) +\n  geom_bar(stat = \"identity\", width=1, color=\"white\", position = \"stack\") +  \n  coord_polar(theta = \"y\", direction = -1, clip = \"off\") +\n  theme_economist(base_family=\"ITC Officina Sans\") + \n  theme(\n    legend.position=\"none\",\n    line=element_blank(),\n    axis.title.x=element_blank(),\n    axis.text.x=element_blank(), #remove x axis labels\n    axis.ticks.x=element_blank(), #remove x axis ticks\n    axis.title.y=element_blank(),\n    axis.text.y=element_blank(),  #remove y axis labels\n    axis.ticks.y=element_blank()  #remove y axis ticks\n  ) + \n  geom_text(aes(y = pos, label = label), color = \"white\", size=6) +\n  scale_fill_economist(labels=NULL)\n\n\n\n\n\n\n\n\n\n\n\nPatient Survival\n\n\nCode\nmimic_data &lt;- mimic_data %&gt;% mutate(\n  survived = case_when(hospital_expire_flag == 1 ~ \"Died\",\n                     hospital_expire_flag == 0 ~ \"Survived\")\n)\n\nggplot(mimic_data, aes(x = survived, fill=survived)) +\n  geom_bar(color=\"white\") +  \n  theme_economist(base_family=\"ITC Officina Sans\") +\n  scale_fill_economist(labels=NULL)\n\n\n\n\n\n\n\n\n\n\n\n\nVital Signs\n\nHeart rate\n\n\nCode\nggplot(mimic_data, aes(x=heartrate_mean)) + \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")\n\n\n\n\n\n\n\nBlood pressure: Median systolic blood pressure 134 mmHg (Q1–Q3: 116–154 mmHg); median diastolic blood pressure 78 mmHg (Q1–Q3: 66–90 mmHg)\n\n\nCode\nggplot(mimic_data, aes(x=sysbp_mean)) + \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")\n\n\n\n\n\n\n\nRespiratory rate\n\n\nCode\nggplot(mimic_data, aes(x=resprate_mean)) + \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")\n\n\n\n\n\n\n\nTemperature\n\n\nCode\nggplot(mimic_data, aes(x=tempc_mean)) + \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")\n\n\n\n\n\n\n\nOxygen saturation: Median 96% (Q1–Q3: 93–99%)\n\n\nCode\nggplot(mimic_data, aes(x=spo2_mean)) + \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")\n\n\n\n\n\n\n\n\nLaboratory Values\n\nWhite blood cell count: Median 10.5 × 10^9 cells/L (Q1–Q3: 7.5–14.5 × 10^9 cells/L)\n\n\nCode\nggplot(mimic_data, aes(x=wbc_mean)) + \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")\n\n\n\n\n\n\n\nNeutrophil count: Median 7.5 × 10^9 cells/L (Q1–Q3: 5.4–11.2 × 10^9 cells/L)\nNOT FOUND IN DATA\n\n\nLymphocyte count: Median 1.7 × 10^9 cells/L (Q1–Q3: 1.0–2.5 × 10^9 cells/L)\nNOT FOUND IN DATA\n\n\nPlatelet count: Median 178 × 10^9 cells/L (Q1–Q3: 125–240 × 10^9 cells/L)\n\n\nCode\nggplot(mimic_data, aes(x=platelet_min)) + \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")\n\n\n\n\n\n\n\nCreatinine: Median 1.0 mg/dL (Q1–Q3: 0.8–1.3 mg/dL)\n\n\nCode\nggplot(mimic_data, aes(x=creatinine_max)) + \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")\n\n\n\n\n\n\n\nBilirubin: Median 0.8 mg/dL (Q1–Q3: 0.5–1.2 mg/dL)\nNOT FOUND IN DATA #### Lactate dehydrogenase: Median 250 U/L (Q1–Q3: 190–330 U/L)\n\n\nCode\nggplot(mimic_data, aes(x=lactate_mean)) + \n  geom_histogram( color=\"#e9ecef\", fill=\"#188bc2\", alpha=0.9, position = 'identity') +\n  theme_economist(base_family=\"ITC Officina Sans\")"
  }
]